WEBVTT

1
00:05:41.350 --> 00:05:42.260
Mario Vega: Hello!

2
00:05:42.440 --> 00:05:43.390
Mario Vega: Hey! Akash!

3
00:05:45.360 --> 00:05:46.110
Akash | ECH: Hello!

4
00:05:46.110 --> 00:05:46.670
Louis: And.

5
00:05:48.080 --> 00:05:48.750
Mario Vega: Hey?

6
00:05:49.790 --> 00:05:57.969
Mario Vega: I wanted to ask if the AI is taking notes today. It's it's much easier when it does, because I don't have to take

7
00:05:58.715 --> 00:06:00.375
Mario Vega: yeah. Notes to myself.

8
00:06:03.935 --> 00:06:06.279
Akash | ECH: Mario, I'm helping with the stream today.

9
00:06:06.430 --> 00:06:08.310
Akash | ECH: so whenever you are ready, let me know.

10
00:06:08.840 --> 00:06:15.020
Mario Vega: Alright, yeah. But I want to ask if the if the notes taking is being handled by the AI or.

11
00:06:15.423 --> 00:06:21.069
Akash | ECH: I don't know. You can ask Pooja, because I don't have access of zoom.

12
00:06:21.070 --> 00:06:21.790
Mario Vega: I see.

13
00:06:21.790 --> 00:06:23.199
Mario Vega: So that's yeah.

14
00:06:24.830 --> 00:06:27.739
Mario Vega: I mean, seems like they are enabled. So.

15
00:06:27.740 --> 00:06:28.100
Akash | ECH: Yeah.

16
00:06:28.445 --> 00:06:30.520
Mario Vega: There should be transcripts somewhere. Yeah.

17
00:08:45.800 --> 00:08:53.910
Mario Vega: hey, everyone. We are waiting on more people to join. This was just a couple more minutes I'll be. I will be leaving today.

18
00:08:54.500 --> 00:08:58.229
Mario Vega: I will share the agenda in the chat just in a few.

19
00:09:49.820 --> 00:09:52.850
Mario Vega: Let's wait. Just one more minute, and we can start.

20
00:10:16.450 --> 00:10:17.396
Saulius Grigaitis | Grandine: Thank you.

21
00:10:30.750 --> 00:10:34.270
Mario Vega: Okay, Josh, I think we can. We can start. Just let me know when we're ready.

22
00:10:36.180 --> 00:10:37.480
Akash | ECH: Yeah, we are live.

23
00:10:38.570 --> 00:10:40.160
Mario Vega: Excellent. Thank you.

24
00:10:40.500 --> 00:10:46.389
Mario Vega: Hello, everyone welcome to Acd number 41. Today's June 23, rd

25
00:10:46.510 --> 00:10:59.559
Mario Vega: and we have a a few items in the agenda and the main point being for second 2 and a couple of discussions regarding Eip, 17 9 0 7.

26
00:10:59.980 --> 00:11:03.870
Mario Vega: So yeah, let's get started. I think we can start with.

27
00:11:04.841 --> 00:11:10.310
Mario Vega: The the 1st item agenda. I should also shared the link in the in the chat.

28
00:11:11.100 --> 00:11:22.689
Mario Vega: Please chime in also, if you if you want to leave comments over there. Yeah. So the 1st item is, Fussaka, 2 Barnabas, I think you're the person that can can

29
00:11:23.150 --> 00:11:23.680
Mario Vega: thank you.

30
00:11:24.159 --> 00:11:29.429
Barnabas: So basically, we are still waiting for client implementations. Therefore, we

31
00:11:29.560 --> 00:11:35.234
Barnabas: look like we're not gonna be able to launch today. Hopefully, tomorrow, we're gonna have enough cl tabs.

32
00:11:35.790 --> 00:11:41.490
Barnabas: let's say that their branch is ready. But as of right now, we only have Yale implementations done.

33
00:11:42.180 --> 00:11:44.659
Barnabas: So we are still waiting.

34
00:11:48.540 --> 00:11:55.759
Mario Vega: Okay, thanks. I also want to show that we have a release for the eels tests.

35
00:11:57.530 --> 00:12:07.850
Mario Vega: and we also have a hive instance running right now. And Spencer, do you know, do you have a summary of which clients are passing or failing? That you want to share

36
00:12:09.940 --> 00:12:11.070
Mario Vega: just in general.

37
00:12:27.870 --> 00:12:33.393
Mario Vega: yeah, if you're talking, you're muted, but otherwise I can. I can share what he relate to me.

38
00:12:34.348 --> 00:12:39.130
Mario Vega: A couple of minutes ago. Basically, only red is passing. If I understood correct correctly.

39
00:12:39.240 --> 00:12:44.100
Mario Vega: the hype tests and the rest of the claims are not either running or passing. That's

40
00:12:44.230 --> 00:12:45.650
Mario Vega: what I understood correctly.

41
00:12:46.295 --> 00:12:47.185
Mario Vega: But yeah,

42
00:12:48.200 --> 00:12:55.572
Mario Vega: we can. I can share the link in a few minutes for everyone to see which clients are passing or failing the tests.

43
00:12:57.000 --> 00:12:57.710
Mario Vega: anyway.

44
00:12:58.520 --> 00:13:00.042
Mario Vega: Is there any

45
00:13:00.942 --> 00:13:05.839
Mario Vega: points that want to be raised about Devnet 2 by any of the clients that we should discuss

46
00:13:06.250 --> 00:13:13.330
Mario Vega: any blockers, any or or any problems with the specifications or implementations that anyone wants to raise.

47
00:13:30.810 --> 00:13:40.679
Mario Vega: We can start with. Execution, client clients. I see ben from another mind. Do you have any updates on the status of W. 2.

48
00:13:41.757 --> 00:13:50.840
Ben Adams: Yeah, everything. Everything's okay. Apart from 7, 9 0, 7, which as mentioned in Acde.

49
00:13:51.040 --> 00:13:53.640
Ben Adams: I'm in the latter half this week.

50
00:13:58.970 --> 00:14:03.410
Mario Vega: Cool anyone from Besu that wants to chime in.

51
00:14:04.070 --> 00:14:07.820
Gabriel Trintinalia | Besu: Yeah, this is ready for devnet, too.

52
00:14:14.130 --> 00:14:19.140
Mario Vega: Any issues, regarding specification or any anything in particular. They want to raise.

53
00:14:19.607 --> 00:14:23.812
Gabriel Trintinalia | Besu: Not not really. No issues. Just waiting for

54
00:14:24.970 --> 00:14:28.169
Gabriel Trintinalia | Besu: The 7 h, 7, 9 0, 7. Conversation.

55
00:14:28.730 --> 00:14:31.309
Gabriel Trintinalia | Besu: But no issues. Yeah. Love for them. That, too.

56
00:14:33.170 --> 00:14:34.323
Mario Vega: Thank you.

57
00:14:35.370 --> 00:14:39.209
Mario Vega: Anyone from Geth Marius, or

58
00:14:39.690 --> 00:14:41.319
Mario Vega: is it Matt? In the call.

59
00:14:48.350 --> 00:14:55.779
Marius: I'm not sure if we have a branch yet, but we have 7, 9 0, 7 implemented, and we will prepare a branch with it.

60
00:15:00.210 --> 00:15:01.000
Mario Vega: Thank you.

61
00:15:01.200 --> 00:15:04.840
Mario Vega: Anyone from from Ref. I see Raman in the call.

62
00:15:04.960 --> 00:15:08.240
Mario Vega: You have any updates on Devna 2 implementation.

63
00:15:08.758 --> 00:15:12.389
Roman: We we posted the wrench earlier today.

64
00:15:18.200 --> 00:15:19.160
Mario Vega: Thank you.

65
00:15:20.092 --> 00:15:24.509
Mario Vega: Anyone from everyone. The status on implementation.

66
00:15:24.730 --> 00:15:31.350
milen | Erigon: Yeah, Hi, yeah, so we're gonna we're finalizing the the branch today.

67
00:15:32.090 --> 00:15:47.850
milen | Erigon: there is one failing test which I'm working on at the moment it's for the block, Max, rop size eip? So yeah, once I have this test passing, then we should be ready.

68
00:15:50.620 --> 00:15:51.350
Mario Vega: Thank you.

69
00:15:54.090 --> 00:15:59.058
Mario Vega: I don't think. Is there anyone else from eels did I miss someone from the eels? Or was

70
00:16:00.170 --> 00:16:06.869
Mario Vega: anyone from the sales. I see. I see, Enrico. Do you have any updates on the sales status?

71
00:16:11.542 --> 00:16:19.087
Enrico Del Fante (tbenr): Yeah, from tech. We have. We are working on on Bpo related things.

72
00:16:20.580 --> 00:16:27.399
Enrico Del Fante (tbenr): and we have a bunch of things to fix already, not following closely this, but hopefully, we

73
00:16:27.700 --> 00:16:30.130
Enrico Del Fante (tbenr): have that sorted out soon.

74
00:16:31.280 --> 00:16:34.599
Enrico Del Fante (tbenr): Part of that. Yeah, nothing more.

75
00:16:38.560 --> 00:16:46.400
Mario Vega: Thank you. Anyone from lighthouse available.

76
00:16:49.590 --> 00:16:51.610
Mario Vega: or status of the new 2.

77
00:17:01.480 --> 00:17:07.760
Mario Vega: And Grandin, I think I see soldiers in the do you have any updates.

78
00:17:08.200 --> 00:17:10.980
Saulius Grigaitis | Grandine: Yeah. So so we kind of have a

79
00:17:11.280 --> 00:17:31.476
Saulius Grigaitis | Grandine: a version that is likely could be a candidate for testing. However, we, it seems we don't have another cl client to test against. So yes, if there is somebody who is very confident in their cl implementation, let us know. We will try to test against

80
00:17:32.230 --> 00:17:34.820
Saulius Grigaitis | Grandine: that other sail client.

81
00:17:38.700 --> 00:17:39.990
Mario Vega: Oh, thank you.

82
00:17:40.470 --> 00:17:46.850
Mario Vega: Let's start. I see, Phil. Is there any updates on the implementation?

83
00:17:52.350 --> 00:18:05.740
Phil Ngo: 8 we've been working on our branch. We have a couple more things here that we need to merge to be definitely. But I'll I'll hand off to Matthew to give updates on those specifically.

84
00:18:12.530 --> 00:18:25.699
MatthewKeil: There we go. Can you hear me now? Sorry phone issues. We're have Prs up for several of the features all that's gonna get merged today, and we'll be ready for tomorrow's launch that Barnabas mentioned.

85
00:18:31.000 --> 00:18:37.550
Mario Vega: Sounds great. Thank you. Anyone from any other cl client available to comment

86
00:18:37.660 --> 00:18:42.050
Mario Vega: on status for Devnet 2 who wants to chime in.

87
00:18:54.660 --> 00:18:58.109
Mario Vega: Casey mentions in the chat that prism is ready for tomorrow lunch.

88
00:19:00.660 --> 00:19:01.450
Mario Vega: Great!

89
00:19:05.520 --> 00:19:08.567
Mario Vega: Alright! Good! Good

90
00:19:10.490 --> 00:19:17.819
Mario Vega: If nothing else. I think we can. Keep the the chat on the status in the R&D Disco, please post

91
00:19:18.299 --> 00:19:26.110
Mario Vega: and ping a any anyone from the team so you can. They can know which image they need to use for. W. 2.

92
00:19:27.140 --> 00:19:35.170
Mario Vega: And yeah, if anyone has any questions about the yield tests also, please ping us either me or Spencer to.

93
00:19:35.590 --> 00:19:39.409
Mario Vega: So we can help you out. Obviously, there's like some

94
00:19:39.520 --> 00:19:46.410
Mario Vega: conversation that is gonna be had today and the the test. But the tests are not gonna change for this segment. 2. Probably they're gonna change for Tabna 3

95
00:19:46.840 --> 00:19:48.520
Mario Vega: in regards to el testing.

96
00:19:50.630 --> 00:19:51.609
Mario Vega: All right.

97
00:19:53.180 --> 00:19:57.351
Mario Vega: Let's jump to the next topic, which is pure dust, testing

98
00:19:58.110 --> 00:19:59.789
jochem-brouwer: Can I just chime in quickly.

99
00:19:59.950 --> 00:20:00.275
Mario Vega: Yeah.

100
00:20:00.890 --> 00:20:04.810
jochem-brouwer: Yeah, because I want to present something about the benchmarks of 7, 9 0 7.

101
00:20:06.460 --> 00:20:19.609
Mario Vega: Yeah, of course, I mean, yeah, of course. I wanted to just clear out if there was any anyone wanted to chime in about your desk or status updates about that. If not, we can just jump into your into into.

102
00:20:19.930 --> 00:20:20.710
jochem-brouwer: Okay. Yeah.

103
00:20:21.010 --> 00:20:25.987
Mario Vega: Yeah. Bernard, do you know who is the person to

104
00:20:26.580 --> 00:20:32.099
Mario Vega: comment on Peerdas? Or do you have anything that we want to discuss regarding Peerdas?

105
00:20:32.640 --> 00:20:35.936
Barnabas: I don't think there's too much to discuss regarding periods. To be honest.

106
00:20:36.870 --> 00:20:51.069
Barnabas: I think we want to launch that net 2. And that's gonna give us the new 4 digit stuff. And then we can start testing. Really, the value capacity changes that have been introduced interrupted, not too.

107
00:20:53.190 --> 00:21:01.110
Barnabas: But at the 1st look Berlin dropped them to already have updated the Cgc values. So

108
00:21:02.310 --> 00:21:12.680
Barnabas: yeah, I'm I'm really just waiting for them to to launch so that we can have a nice new network to test everything with.

109
00:21:16.010 --> 00:21:16.450
Mario Vega: Perfect.

110
00:21:16.450 --> 00:21:20.660
Barnabas: Maybe some side lips. Maybe maybe some side lips have something.

111
00:21:21.880 --> 00:21:25.163
Mario Vega: Yeah, they mentioned in the chat that they want to present.

112
00:21:26.840 --> 00:21:32.549
Mario Vega: yeah, if you want to chime in some lives. Yes, please. Just go ahead, we can. We can.

113
00:21:32.770 --> 00:21:33.370
Mario Vega: Yep.

114
00:21:33.370 --> 00:21:44.598
J Sunnyside Labs: Yeah. Hi, I will just briefly talk about the analysis that we found from Berlin interrupts, and like the days few days before that. So

115
00:21:45.908 --> 00:21:48.860
J Sunnyside Labs: yeah, I think like.

116
00:21:48.990 --> 00:21:57.040
J Sunnyside Labs: for some of us who went to Berlin knows that. Most of the cl clients

117
00:21:57.270 --> 00:22:04.709
J Sunnyside Labs: we test like for all Dcl. Clients. We tested single cl devnets. So we ran like

118
00:22:04.840 --> 00:22:09.830
J Sunnyside Labs: 18 devnets in total at the end, and we found that

119
00:22:10.270 --> 00:22:18.129
J Sunnyside Labs: all the cl clients, except for Nimbus at the time, were able to reach up to 72 blocks per block.

120
00:22:19.460 --> 00:22:22.645
J Sunnyside Labs: Congratulations and

121
00:22:24.200 --> 00:22:32.470
J Sunnyside Labs: We made analysis on where where were the bottlenecks to please have a look? Every cl

122
00:22:32.820 --> 00:22:36.809
J Sunnyside Labs: has some bottlenecks when it reaches to 72 blobs.

123
00:22:38.450 --> 00:22:41.008
J Sunnyside Labs: and for the next up

124
00:22:41.970 --> 00:22:51.039
J Sunnyside Labs: we may do network bandwidth limiting tests, but the metrics we get it for full nodes.

125
00:22:51.790 --> 00:23:01.809
J Sunnyside Labs: Don't look that great grand dynamic lighthouse. Full notes consume 20 megabits per second on average.

126
00:23:02.300 --> 00:23:07.899
J Sunnyside Labs: and others were above 80 or 100 megabits per second.

127
00:23:08.930 --> 00:23:13.440
J Sunnyside Labs: So this was the major bottleneck we found.

128
00:23:17.210 --> 00:23:23.780
J Sunnyside Labs: Yeah, other than that there are some, some others that are also part of the report, which

129
00:23:24.010 --> 00:23:29.379
J Sunnyside Labs: I wouldn't say it now, because they are too long. But please have a look.

130
00:23:37.525 --> 00:23:39.460
J Sunnyside Labs: How could yeah.

131
00:23:40.723 --> 00:23:43.060
Mario Vega: I am sorry I cut you off.

132
00:23:43.480 --> 00:23:44.350
Mario Vega: Please go ahead.

133
00:23:44.350 --> 00:23:44.920
J Sunnyside Labs: Oh.

134
00:23:45.868 --> 00:23:57.439
J Sunnyside Labs: I think Anzker has a question about stability on the realistic networking. So we mainly focus on the

135
00:23:58.069 --> 00:24:03.780
J Sunnyside Labs: limitation side. So at the very worst case, what happens? So

136
00:24:04.573 --> 00:24:12.150
J Sunnyside Labs: we. At the moment we didn't limit any network bandwidth, and we are about to begin

137
00:24:12.994 --> 00:24:15.370
J Sunnyside Labs: constraint constraining the network.

138
00:24:15.950 --> 00:24:23.510
J Sunnyside Labs: But what we found so far is that we probably have the network

139
00:24:23.730 --> 00:24:27.420
J Sunnyside Labs: failing much before than 72 blocks per block.

140
00:24:27.970 --> 00:24:32.209
J Sunnyside Labs: because full nodes are consuming a lot more than expected.

141
00:24:42.320 --> 00:24:47.789
Ansgar Dietrichs: Well, and maybe just to briefly ask, do we? How close are we to understanding? Why? Why, that is the case.

142
00:24:52.060 --> 00:24:53.942
J Sunnyside Labs: In the report.

143
00:24:54.770 --> 00:25:04.419
J Sunnyside Labs: I mentioned that number of column verification is a lot for some of the Cls. That might be the issue.

144
00:25:05.360 --> 00:25:06.610
J Sunnyside Labs: and

145
00:25:09.090 --> 00:25:17.250
J Sunnyside Labs: Not sure why, but it was generally increasing linearly when number of globes increased

146
00:25:21.880 --> 00:25:25.740
J Sunnyside Labs: and Els didn't have much impact on that.

147
00:25:26.840 --> 00:25:32.630
Raúl Kripalani: Yeah. So 1 1 question here, were you testing only with publicly available blobs.

148
00:25:34.430 --> 00:25:35.350
J Sunnyside Labs: Yes.

149
00:25:36.250 --> 00:25:45.310
Raúl Kripalani: Right and were you able to trace down if the failure to propagate, or maybe the amplification was at the El level or at the Cl level.

150
00:25:49.296 --> 00:25:52.423
J Sunnyside Labs: Sorry I didn't quite get the question.

151
00:25:52.870 --> 00:26:05.330
Raúl Kripalani: Yeah. So there's so blobs propagate. So if you're testing with public blobs, there's type 3 transactions that propagate in the El blob pool. Right? So what we're trying to kind of like figure out, maybe as a 1st step is

152
00:26:05.888 --> 00:26:15.060
Raúl Kripalani: whether the type 3 transactions failed to propagate, or whether the C they propagated. But somehow the Cl. Was kind of like

153
00:26:15.610 --> 00:26:17.850
Raúl Kripalani: amplifying traffic too much, or something.

154
00:26:20.383 --> 00:26:23.470
J Sunnyside Labs: I I think the letter was the case. So

155
00:26:24.014 --> 00:26:30.089
J Sunnyside Labs: we also tried to see El Usage, but it didn't increase that much. So

156
00:26:33.640 --> 00:26:37.330
J Sunnyside Labs: get. The notable difference was between

157
00:26:37.982 --> 00:26:43.652
J Sunnyside Labs: lighthouse grander and compared to others. So others were using

158
00:26:44.800 --> 00:26:50.659
J Sunnyside Labs: like 80 megabits per second, more on average, than lighthouse and Grandin.

159
00:26:55.640 --> 00:27:11.280
J Sunnyside Labs: what did you mention was linearly, I'm sorry. Thank you. Linearly scaling with blobs. So we used spammer to increasing to increase blobs over time like

160
00:27:11.700 --> 00:27:20.000
J Sunnyside Labs: plus one block every 5 min. And then, when we say, as time passes, we say,

161
00:27:21.560 --> 00:27:26.479
J Sunnyside Labs: The network traffic was also linearly increasing.

162
00:27:28.280 --> 00:27:30.700
Csaba Kiraly: Yeah. But that's that's normal with actually with

163
00:27:31.360 --> 00:27:34.020
Csaba Kiraly: like, that's what we expect, isn't it?

164
00:27:34.260 --> 00:27:40.199
Csaba Kiraly: That's why I'm asking, because because the many things is the only design. Many things are, you know, just getting with the number of drops.

165
00:27:41.030 --> 00:27:46.309
Csaba Kiraly: So that that's why I want to understand. If if you've seen something unexpected, or it's it's actually what we expect.

166
00:27:47.180 --> 00:27:56.350
Csaba Kiraly: So the the message sizes, the column sizes linearly growing. And and anything done on the column level is, is just linearly growing

167
00:27:56.600 --> 00:27:59.669
Csaba Kiraly: with the increase of number of blobs, with the with the one d. Design.

168
00:28:01.730 --> 00:28:08.940
J Sunnyside Labs: Yes, but I don't think we expected this much of network usage, don't we?

169
00:28:10.850 --> 00:28:17.600
Csaba Kiraly: Yeah, no. The question is, was it? Was it the network usage generally scaling? And it's more than what you would expect even with

170
00:28:17.760 --> 00:28:21.580
Csaba Kiraly: with foil blobs. You just don't notice, because it's

171
00:28:21.710 --> 00:28:30.119
Csaba Kiraly: below this threshold of of noticing it. Or so, if if that's why, because it was nearly scaling, that means that it's it's just more than

172
00:28:30.670 --> 00:28:35.600
Csaba Kiraly: even with small number of drops than what we would expect rather than.

173
00:28:35.600 --> 00:28:56.469
Raúl Kripalani: Yeah, I think I think we need to really dissect these results closely. So I can from the peer to peer networking team. We can do that. We'll look closely at the report, and we'll we'll chat with with the Sunnyside labs team to get to the bottom of what might be happening here. There's also kind of like a duplicate effect

174
00:28:56.700 --> 00:29:19.019
Raúl Kripalani: factor here. I don't know what the status of implementations is, with regards to the latest improvements that that we've introduced, things like I don't want, and and other things which, by the way, would only be visible really, when there is a network latency which I don't think in this particular case there was right. There was no specific network configuration enforced here, as far as I understand.

175
00:29:20.040 --> 00:29:22.830
Csaba Kiraly: No, no limit at all.

176
00:29:23.260 --> 00:29:24.340
Csaba Kiraly: There's justification.

177
00:29:24.620 --> 00:29:29.729
Csaba Kiraly: So you have the the geographic distributed. So you have some latency effect. But but.

178
00:29:30.070 --> 00:29:37.610
Raúl Kripalani: Right? Yeah, yeah, yeah. But it was all all on the same as right. Probably run on aws, or something like that.

179
00:29:39.410 --> 00:29:40.180
J Sunnyside Labs: On, digital.

180
00:29:40.180 --> 00:30:01.289
Raúl Kripalani: I think, yeah, right? Yeah. So it's not. Re, it's not real world. Effects. I I do think that we need to look at exactly the commits of the networking stack that the clients were running at that point in time, and the implementation of some of the latest optimizations. But yeah, this this is. This is really good flag. We'll we'll look into into this.

181
00:30:02.830 --> 00:30:07.069
J Sunnyside Labs: Yeah, sure. Yeah, we will collaborate with you guys.

182
00:30:07.360 --> 00:30:08.660
J Sunnyside Labs: Thank you so much.

183
00:30:13.918 --> 00:30:23.120
Mario Vega: Raul do you mention? There's something our engine be get block speed to that you want to talk about. I think we can go into that first, st and then come back to 7, 9 0 7.

184
00:30:23.290 --> 00:30:52.530
Raúl Kripalani: Yeah, sure. So yeah, I think it's, it's the relevant context. So yeah, so I submitted, this pr, earlier, today, this basically proposes a small change to get blobs p. 2. To restore the previous functionality. Where get blobs would return partial hits so when in v. 2, we switch to an all or nothing behavior, where, if a single blob is missing from the El Blob pool, then the whole response would be a null

185
00:30:52.530 --> 00:31:11.150
Raúl Kripalani: which made sense from the perspective of the current design of pure das, which can only utilize full columns. So if you're missing a single blob, then you're missing a cell in that column, and there's nothing that you can do. So you have to wait for the network to receive that column from the network on the Cl side. But we're working on a bunch of

186
00:31:11.551 --> 00:31:40.259
Raúl Kripalani: Optimizations at the peer to peer networking level. Specifically at the gossip sub level that would allow us to do to send cell level deltas. And of course, there's like we're prototyping these, and there's a bunch of testing that we need to do to figure out specifically the timing conditions that would lead to good results. In in good outcomes in in this case. But yeah, the idea is to make whatever is available in the El useful

187
00:31:40.661 --> 00:31:47.880
Raúl Kripalani: to the to the Cl so that we can leverage optimizations that we can introduce without

188
00:31:48.306 --> 00:31:56.429
Raúl Kripalani: forts in the network. So this, these would be implementation level optimizations. And and yeah, so this gives us

189
00:31:56.450 --> 00:32:06.019
Raúl Kripalani: so basically, what this Pr is doing is proposing that we introduce a partial response, flag to the request payload of get blobs, p. 2.

190
00:32:06.030 --> 00:32:20.090
Raúl Kripalani: When that is true, we would return the blobs that are available and null for the blobs that aren't, and if that is false, we would just continue with the current behavior which is returning a null for as a as a global response.

191
00:32:20.371 --> 00:32:39.810
Raúl Kripalani: We would. At this stage we would probably deploy Cls would be using the false value here. But then, as we roll out these optimizations as we and we agree on them and we implement them, we test them. And and we see the lead to good results. They could switch to a true and start utilizing the data that's available in the in the block pool.

192
00:32:41.070 --> 00:33:06.439
Raúl Kripalani: And the reason that we that we suggested Mikhail had a question of why don't we use partial response through as the default logic? The reason is that currently the Cl won't be able to utilize the the partial responses. But we do want the flexibility so that we can switch to to that behavior to the the partial response, false behavior when?

193
00:33:06.950 --> 00:33:13.650
Raúl Kripalani: sorry the the other way around, I think. Yeah, so that we can switch to partial response whenever whenever we should. The optimization.

194
00:33:20.380 --> 00:33:40.120
Raúl Kripalani: Yeah. So Marius, the idea. So I see you. You asked, why don't we just return partial responses, anyway? And the reason is that we can't use those partial responses. So would be currently would be on undergoing the cost of survey on both sides for nothing. So so yeah, this flag would would make it

195
00:33:41.180 --> 00:33:42.760
Raúl Kripalani: would make it dynamic.

196
00:33:48.200 --> 00:33:58.009
Marius: Yes, but I think in this, in this case I would just say, let's make this the default behavior and not introduce a another flag

197
00:33:58.370 --> 00:34:00.489
Marius: like the cost of.

198
00:34:00.490 --> 00:34:01.140
Raúl Kripalani: That's.

199
00:34:01.780 --> 00:34:06.010
Marius: So it's not that much, and.

200
00:34:07.160 --> 00:34:07.630
Raúl Kripalani: Excellent.

201
00:34:07.630 --> 00:34:15.940
Marius: Like my-, my personal preference would just be to make this the default behavior as we had it before the spec changed.

202
00:34:16.120 --> 00:34:24.660
Marius: and let the Cls filter out while I look at the responses and see

203
00:34:24.889 --> 00:34:28.120
Marius: how many of those blobs are

204
00:34:28.449 --> 00:34:34.900
Marius: they- they- they do not, if they have all of them, and if not, we'll just not use it.

205
00:34:35.229 --> 00:34:46.679
Raúl Kripalani: Functionally speaking, that's good that that works for us. I was just kind of like going the extra mile here, just just in case we want to serve save the survey cost. If we get to say, for example.

206
00:34:47.039 --> 00:35:03.471
Raúl Kripalani: 36 blobs before this optimization, and we realized that the base. 64 encoding is wait like I don't know. It's adding, like maybe 20 ms, 30 ms, or something like that. And we kind of like, you know on on each side. I don't know how much that is. I haven't benchmarked it.

207
00:35:04.379 --> 00:35:11.839
Raúl Kripalani: but but yeah, I would be currently returning. Those blobs would not be useful right now on the cl, anyway. So that's why

208
00:35:11.989 --> 00:35:13.779
Raúl Kripalani: I thought this would be a simple.

209
00:35:13.889 --> 00:35:19.499
Raúl Kripalani: simple solution to save extra work. But we both know that it's not useful.

210
00:35:28.639 --> 00:35:34.069
Raúl Kripalani: but I don't have a preference just returning partial responses by default and removing the flag is good enough for us.

211
00:35:38.100 --> 00:35:49.000
Mario Vega: I think the just want to. Raise the point that maybe if we keep the discussion into the Pr, please just chime in. And just to get this either merged or

212
00:35:49.610 --> 00:36:01.409
Mario Vega: just to not block this this pr, do you need any feedback in a specific role that will help get this pr other measure close.

213
00:36:02.576 --> 00:36:04.649
Raúl Kripalani: Yeah. So I would just like a

214
00:36:04.780 --> 00:36:28.650
Raúl Kripalani: outcome here. I think we it's a very simple issue to to agree on I. And it seems like, based on the reactions, on the chat. I see that most people seem to be agreeing that we just remove the option so I would just what I'm gonna do. If if people think it's fine, I'm just gonna rehash this Pr and just remove the the flag altogether and just change the behavior

215
00:36:31.730 --> 00:36:34.620
Raúl Kripalani: to get to to a solution here is just very simple.

216
00:36:37.180 --> 00:36:41.340
Mario Vega: Any opposition on that. Resolution.

217
00:36:51.280 --> 00:36:54.860
Mario Vega: Okay, that sounds good. Then. Thank you.

218
00:36:55.375 --> 00:37:06.734
Mario Vega: If nothing else. I wanted just to. Raise this point by answer in the in the chat regarding timeline. Answer. Do you want to comment

219
00:37:07.600 --> 00:37:09.224
Mario Vega: Or ask the question just

220
00:37:10.290 --> 00:37:38.240
Ansgar Dietrichs: Yeah, I mean, I'm not sure. Feel free to tell me if this is just not a good fit to ask on this call. I'm just wondering right now. Whether we are working towards like a specific list like like check checklist or something of remaining stages here between now and being ready to basically start forking the test nets in terms of Ps, like what exactly we need to to see, also to inform us our our choice on on the the exact details of which values we we use for the Bpo, and if we have a specific like checklist in mind, then

221
00:37:38.677 --> 00:37:48.829
Ansgar Dietrichs: a, is that written down somewhere, something that that that could be linked, and B what the rough timeline, I mean. Again, optimistic, of course, would look like for this.

222
00:38:03.658 --> 00:38:11.410
Mario Vega: I don't know who the person to answer this. But maybe just to bring some context. Barnabas, do you have any like comments on

223
00:38:11.750 --> 00:38:13.889
Mario Vega: on this that you want to bring up.

224
00:38:14.621 --> 00:38:19.430
Mario Vega: It. It's a sense to me that we want stability right in on at least, yeah.

225
00:38:19.430 --> 00:38:20.539
Mario Vega: as before. Yeah.

226
00:38:20.540 --> 00:38:27.500
Barnabas: Yeah, we we ideally want a table spec, like a frozen spec, which it seems like we don't even have right now.

227
00:38:28.322 --> 00:38:36.419
Barnabas: We most likely gonna be going into the next 3, and possibly 2 weeks from now, or maybe even a bit later.

228
00:38:36.720 --> 00:38:43.830
Barnabas: By then all these questions that were raised today should be clarified. And then we can actually discuss about

229
00:38:44.160 --> 00:38:46.080
Barnabas: what else do we actually need?

230
00:38:46.270 --> 00:38:52.169
Barnabas: Because we are now at a happy case scenario, where everything is working with limited number of blobs

231
00:38:52.949 --> 00:39:08.230
Barnabas: testing Cpo numbers is impossible to do on any test net. So we're gonna need to kind of do analysis after before full and determine the Bpo values based on that.

232
00:39:12.068 --> 00:39:25.639
Francesco: One thing is, I I think, as soon as definite, the the next 7 is disabled like without necessarily waiting for Devnet 3. Even if let's say, I don't know, this small change is is made that we just discussed, we should probably

233
00:39:26.390 --> 00:39:27.640
Francesco: start like

234
00:39:27.940 --> 00:39:43.720
Francesco: more well doing more of the the kind of tests that now the the Sunnyside Labs people are doing and and just really like, I mean, yes, we probably cannot fully figure out the the safe number of of lobs for the Ppo schedule. But just generally we should

235
00:39:44.120 --> 00:39:59.632
Francesco: start working towards like understanding what the bottlenecks on the Cl. Network inside are better than than we do now. So part of that is just like following up on the I think you know the questions that were raised by the report. But yeah, ideally, it would be nice if we can, maybe

236
00:40:00.250 --> 00:40:15.899
Francesco: at some point start in parallel, like spinning up some like bigger devnets than even, you know, 50 nodes. And and maybe yeah, just just start understanding really what the networking level or like verification level kind of bottlenecks are.

237
00:40:16.509 --> 00:40:30.020
Francesco: I think then I mean, once we understand that it's there's not. It's it's hard to say timelines, because then it it might take like we don't know how long it's gonna take to address those bottlenecks, but

238
00:40:30.160 --> 00:40:40.449
Francesco: at least like having, like a complete list of all the things that are not fully ready in terms of the the networking performance would be the start, I guess.

239
00:40:42.564 --> 00:40:48.759
Barnabas: Yeah, sure. But I also feel like fundamentals should be working before we doing any kind of tests.

240
00:40:49.210 --> 00:40:52.481
Barnabas: So like manager custody is a fundamental feature

241
00:40:53.530 --> 00:41:01.389
Barnabas: back feeling, in my opinion, should be a fundamental feature that all Cl support and currently none of them support it. So

242
00:41:01.550 --> 00:41:07.860
Barnabas: we still have a bunch of fundamental topics that are not yet implemented by many

243
00:41:08.480 --> 00:41:14.010
Barnabas: and doing stress tests and doing edge case testing already when

244
00:41:14.170 --> 00:41:19.050
Barnabas: we can't even do backfills is seems a bit stretched out.

245
00:41:21.140 --> 00:41:33.940
Francesco: Yeah, I agree. I think we should 1st have at least 7 or 2 being stable and in the meantime, yeah, I mean the same kind of reports and tests that are being done now can can still be done, and we can investigate them. But yeah, I think

246
00:41:34.835 --> 00:41:39.329
Francesco: other than maybe you know what we discussed today. We shouldn't make any

247
00:41:39.792 --> 00:41:45.120
Francesco: bigger changes to to the spec. So hopefully, like Devnet to being stable is, is like

248
00:41:45.440 --> 00:41:48.480
Francesco: the, you know, kind of the last point at which

249
00:41:49.850 --> 00:41:55.539
Francesco: like. Once we see them in the 2 stable, then we really can start doing these other tests? More seriously.

250
00:42:00.170 --> 00:42:05.650
Ansgar Dietrichs: And maybe one last follow up question. Then that's that's helpful. Do we expect to

251
00:42:05.910 --> 00:42:20.080
Ansgar Dietrichs: basically just as long as, say, the once we start just testing really out on the realistic networking conditions if we get and and and on foulu and basically we get some sort of numbers that with current client behavior we can support

252
00:42:20.220 --> 00:42:42.670
Ansgar Dietrichs: is the expectation. As long as those don't look horrible. We would just then immediately base the Bpo. Numbers on top of that and start rolling towards Maine, moving towards main rollout. Or do we expect to basically try to have some one or several rounds of of feedback, of trying to basically unblock the bottlenecks and get to highest possible performance, and only then lock in the the Bpo numbers.

253
00:42:43.290 --> 00:42:53.130
Ansgar Dietrichs: And and do we know, like where that kind of decision cut off would be in terms of what numbers we would have to see, to immediately move towards Rollout versus keep it improving.

254
00:42:55.190 --> 00:42:58.679
Ansgar Dietrichs: or will we just make these decisions on the fly as we get closer to that.

255
00:43:07.170 --> 00:43:15.750
Raúl Kripalani: Think, from from my perspective, what would be useful is to continue working, for example. So both kinds of testing that Barnabas and and

256
00:43:16.197 --> 00:43:40.309
Raúl Kripalani: Francesco were mentioning are are useful but also like getting finding those bottlenecks and the networking level bottlenecks, and specifically those that are resulting from maybe the design of things that like, then place specific workloads on the network and so on, like those would be really, really important to find as soon as possible. So the kind of benchmarking that Sunnyside labs is doing is very useful at this stage. So we just need to get to more realistic

257
00:43:40.645 --> 00:43:59.759
Raúl Kripalani: scenario so that we can start seeing those issues, larger scales and also really understanding what the what, the version of clients that were used and the specific configurations at the at the networking level machine level, and so on, were as well. I would say from like this, what is what would happen next?

258
00:44:00.115 --> 00:44:16.840
Raúl Kripalani: I would assume that pure das in itself, nothing about the design is likely gonna change so based on what we discover, there will be specific, maybe implementation level changes that we can adjust optimizations. We can review some code and like some some parts of

259
00:44:16.890 --> 00:44:36.590
Raúl Kripalani: how the implementations are using the networking stack the networking stack itself, maybe even like machine tuning. There's a bunch of things that we can do there that would potentially unlock higher, higher numbers and higher levels of scale, and based on that, I would assume that we that we would set the Ppo schedule.

260
00:44:41.890 --> 00:44:52.340
Mario Vega: Alright, just to try to finalize the discussion here, I think. We can see the outcome of that 2 and then continue to discuss this in Acdc.

261
00:44:52.890 --> 00:44:55.619
Mario Vega: if that sounds sounds good by everyone.

262
00:44:58.500 --> 00:44:59.370
Mario Vega: All right.

263
00:45:01.710 --> 00:45:06.779
Mario Vega: Cool. Thanks. Thanks, Raul and Sunnyside lights for the for the discussion.

264
00:45:07.495 --> 00:45:10.409
Mario Vega: Yeah. And now, going back to

265
00:45:10.800 --> 00:45:22.700
Mario Vega: Eip 79 0 7, I think Johan wanted to discuss the current status and the findings of benchmarking. Johan, do you want to share in.

266
00:45:23.380 --> 00:45:26.240
jochem-brouwer: Yes, let me quickly share my screen.

267
00:45:28.830 --> 00:45:30.510
jochem-brouwer: Can you guys see my screen.

268
00:45:32.590 --> 00:45:33.260
Mario Vega: Yes.

269
00:45:33.660 --> 00:45:35.209
jochem-brouwer: And also my mouse.

270
00:45:35.910 --> 00:45:36.820
Gabriel Trintinalia | Besu: Yes.

271
00:45:36.820 --> 00:45:37.320
Mario Vega: Yes.

272
00:45:37.320 --> 00:45:49.619
jochem-brouwer: Very cool, very cool, all right, Marius and me. We made some. We did some 7, 9 0 7 benchmarks on the Shadow Fork of Mainnet. So we can't test bigger contacts there because we can't deploy those.

273
00:45:49.950 --> 00:45:54.099
jochem-brouwer: What we did there is we sent transactions with a gas limit of 30 million.

274
00:45:54.620 --> 00:46:09.259
jochem-brouwer: and we have 6 targets, one of xcode size small. What we do here is we call xcode size on the target. This calls for more than 11,000 unique contracts with a size of 12 GB,

275
00:46:09.520 --> 00:46:16.779
jochem-brouwer: also for xcosize big. This targets the maximum contract size currently which you can currently deploy of 24 kB.

276
00:46:16.930 --> 00:46:21.190
jochem-brouwer: The reason for this is that if you have these 2 results, you could extrapolate

277
00:46:21.310 --> 00:46:27.250
jochem-brouwer: the results to somewhat see what would happen if it would. For instance, double this this size.

278
00:46:27.710 --> 00:46:56.259
jochem-brouwer: What we also did is you did a statical on these small contracts, a statical on these big contracts. So 12 kB and 24 kB. And this includes jump test analysis. What it does is at the start of the contract. It will jump to the end of the contract, and this contract itself is completely filled with jump tests. So this is so you should really analyze this and also for these contracts. Each contract is unique. So it is forced to do this gym test analysis every time.

279
00:46:57.030 --> 00:47:10.350
jochem-brouwer: and also extra. This is statical, small, aesthetical, big. And this goes into unique context, and it does not do the jump test analysis there, because it immediately executes the stop of code. So it does not have this jump analysis.

280
00:47:10.830 --> 00:47:16.250
jochem-brouwer: Well, here are the results. These are the average execution times of every client.

281
00:47:17.050 --> 00:47:23.330
jochem-brouwer: Well, you can see that Basu, for instance, is well, yeah, the slowest. Apparently

282
00:47:24.130 --> 00:47:33.119
jochem-brouwer: there are also some some very weird resource. For instance, Gef has for xcode size, small and xcode size big. It doesn't a slower average or sorry

283
00:47:33.300 --> 00:47:39.469
jochem-brouwer: a faster execution time for the big xcode size contract. So that's a very weird result.

284
00:47:39.780 --> 00:47:58.970
jochem-brouwer: But yeah, what we can see this is already like somewhat alarming. I would say that Bezo has a well, let's say more than 800 ms execution time for these statical big contracts, and this is only for a guesstimate of 30 million. So it's not like our target, which I think is 60 million or something like that.

285
00:47:59.694 --> 00:48:02.519
jochem-brouwer: So, yeah, this is, this is maybe somewhat alarming.

286
00:48:02.800 --> 00:48:27.800
jochem-brouwer: We also have, like the worst case blocks. This was more than a second for xcode, small for Bizu, almost a second, for never mind, for the statical small jug test. Well, this is, of course, also a interesting result, because we would assume that the big contracts will be the well. The worst case here Gef. Is more than half a second, for statical big

287
00:48:27.940 --> 00:48:38.150
jochem-brouwer: Ref is well, 250 ms for xcode. The size big and irregon. The worst case is 130 ms for statical, big.

288
00:48:38.800 --> 00:48:53.659
jochem-brouwer: and yeah, just a disclaimer. These results are preliminary because we also need to dig into the behavior at 45 million guests, 60 million guests, 100 million guests. And we also need to see what happens. And for this we would need a new fork.

289
00:48:54.670 --> 00:49:01.730
jochem-brouwer: because we will need clients which would support deploying contracts which are bigger than 24 kB

290
00:49:01.870 --> 00:49:10.120
jochem-brouwer: to analyze this behavior. And of course, also this weird behavior. For ef why is this X code a size small? Well.

291
00:49:10.560 --> 00:49:19.890
jochem-brouwer: worse than the xcode size, big. And also like we have to look into the client's caching behavior because this could also well influence the the behavior.

292
00:49:20.230 --> 00:49:30.700
jochem-brouwer: And as a general point, I think the testing team has also seen this. Currently, there are some well, some edge cases in the eap and in the implementation, and also for the for the to write the tests that are

293
00:49:30.800 --> 00:49:34.549
jochem-brouwer: not. Yeah, not very well specified in the erp. Yet

294
00:49:34.720 --> 00:49:40.370
jochem-brouwer: so we should specify these these things in the eap to make this clear.

295
00:49:40.710 --> 00:49:46.210
jochem-brouwer: And I'm seeing a question in the chat. How are the tests executed?

296
00:49:46.330 --> 00:49:49.269
jochem-brouwer: Yeah, that's various comments on this.

297
00:49:50.000 --> 00:49:53.110
jochem-brouwer: Maybe Marius has to add something here, or someone has a question.

298
00:50:03.870 --> 00:50:07.368
Mario Vega: I think I think Marius already responded, that it was running perfnet

299
00:50:07.660 --> 00:50:08.200
jochem-brouwer: Yes.

300
00:50:08.540 --> 00:50:09.400
Mario Vega: Yeah, okay.

301
00:50:12.490 --> 00:50:18.890
jochem-brouwer: Yeah. So just as the disclaimer, we need more time to look into this. But yeah, as we've seen.

302
00:50:19.080 --> 00:50:23.029
jochem-brouwer: yeah, the times are very well.

303
00:50:23.600 --> 00:50:25.829
jochem-brouwer: not not super nice for some clients. So, yeah.

304
00:50:28.220 --> 00:50:31.799
jochem-brouwer: and this is so without any coda size increase

305
00:50:32.220 --> 00:50:34.730
jochem-brouwer: for 24 kB context. Max.

306
00:50:41.210 --> 00:50:52.130
Łukasz Rozmej: If I can weigh in it might depend on which hardware are you running? Because a lot of jump desk can be vectorized. And that depends on the hardware.

307
00:50:52.290 --> 00:51:09.820
Łukasz Rozmej: I expect this was hardware support x 86, supporting awx, 2. Hardware. So 256 bit instructions you can optimize for also awx 512 and for arm 70 instructions. But the results will vary.

308
00:51:10.607 --> 00:51:23.880
Łukasz Rozmej: You can optimize potentially the jump desk analysis for worst for best the best case average case, that it will have more jump desk, or more push of codes.

309
00:51:24.340 --> 00:51:39.699
Łukasz Rozmej: And if you optimize for one, you kind of potentially are trade-offing for the second one. So that's another thing that some clients can optimize for one, some clients for the other. But it's hard to have something that

310
00:51:39.920 --> 00:51:47.910
Łukasz Rozmej: would work for both especially fast. So that that's another potential problem, that the worst case

311
00:51:48.380 --> 00:51:54.789
Łukasz Rozmej: is one or the other right, or either all pushes or no jump desk or full jump desk without pushes.

312
00:51:55.690 --> 00:51:57.250
Łukasz Rozmej: What else?

313
00:51:58.150 --> 00:52:07.889
Łukasz Rozmej: Yeah, that's that's kind of it that it can be hardware dependent. It can be implementation dependent on on the optimization side. A bit

314
00:52:08.050 --> 00:52:09.969
Łukasz Rozmej: just wanted to give more context.

315
00:52:10.780 --> 00:52:29.769
jochem-brouwer: Yeah, okay, just to weigh in on that. You are right because you can optimize for either the push up code or for the gym test opcode. But of course, what we want to test here is the worst case. So yeah, then then I will just add a gym test analysis for the push also, so that you can't really optimize for one or the other, you will still get like the worst case out of it.

316
00:52:29.980 --> 00:52:31.740
jochem-brouwer: Yeah, in the end.

317
00:52:33.570 --> 00:52:40.069
Łukasz Rozmej: Yeah, but it would be good to have benchmarks for both cases, because it can be very dependent on which client chooses.

318
00:52:40.400 --> 00:52:42.799
jochem-brouwer: Yeah, I agree, I will add this, yeah, good point.

319
00:52:44.830 --> 00:52:47.849
Łukasz Rozmej: Also we will be adding

320
00:52:48.620 --> 00:52:58.210
Łukasz Rozmej: additional metadata and save jump this, probably to to disk along with some kind of index for size.

321
00:53:01.480 --> 00:53:15.330
jochem-brouwer: Yeah, cool sounds good. Yeah. Just to mention also, also for 7,907. This has the implicit behavior that you have to cache either caches on disk or caches in memory like this code hash to decoder size.

322
00:53:15.650 --> 00:53:25.210
jochem-brouwer: Lookup table, because if you don't do this, then well, you can, you will really run into issues that you have to to read massive data from the disk.

323
00:53:25.500 --> 00:53:32.900
jochem-brouwer: And this is really implicit, that we that every client adds this, I will also add a test to East for this to test.

324
00:53:39.620 --> 00:53:43.980
Ansgar Dietrichs: Yeah, maybe then I can, because I wanted to basically talk about that specific thing.

325
00:53:44.160 --> 00:53:55.399
Ansgar Dietrichs: So to me, basically that the nice thing in a way, at least, about the jump test. Worst case is that the jump test is should at least in principle, not be a new worst case caused by the cip.

326
00:53:55.480 --> 00:54:20.429
Ansgar Dietrichs: because the idea is that the Ap charges proportional to the code size. Right? So now, a contract of double the size should also cost double the amount of gas, and the problem that we just have is that you can only really enforce that once you know the size of the contract. But you run the jump test analysis after you know the size of the contract. So in principle, by this point, there's no way of dosing the client basically like, yes, jump test runs slower if the size is larger. But basically this is, then

327
00:54:20.560 --> 00:54:32.890
Ansgar Dietrichs: if there is an issue here, then it's just an existing performance issue which we still have to take seriously and tackle. But it's not a new issue caused by the Erp around jump test, at least, whereas the one thing that you can't so easily

328
00:54:32.970 --> 00:55:01.510
Ansgar Dietrichs: bound by this pricing is the reading from from the database itself. As you just said. So basically like, There, you really need. You need to basically choose a strategy, either. You just accept that now, your worst case got worse, maybe not quite twice twice, because it's still a sequential read. So it's maybe a bit faster than twice as bad. But we need metrics on this right, like how bad the performance regulation gets, and they just accept that or you implement this optimization that you have a index that you keep.

329
00:55:01.530 --> 00:55:14.690
Ansgar Dietrichs: That is a mapping of contract to contract of address or hash to the size. My big argument here would be that I personally think we can't treat this index as an afterthought. I think we need

330
00:55:14.690 --> 00:55:32.689
Ansgar Dietrichs: to have a agreed upon standardized behavior across all clients. Because if we don't, basically, then again, we have now very variable worst cases. I think we need to have an agreed upon. We need to have this decision. Basically like, do we want this index mandated in all clients by the time Fusaka rolls out or not.

331
00:55:32.810 --> 00:55:37.250
Ansgar Dietrichs: and and in general, I personally, if if we lean towards

332
00:55:37.360 --> 00:55:39.710
Ansgar Dietrichs: doing it, then I I have some

333
00:55:39.820 --> 00:55:45.509
Ansgar Dietrichs: pretty severe concerns around this index. In general. I personally really dislike that it's not

334
00:55:45.850 --> 00:56:13.979
Ansgar Dietrichs: in any way enshrined, meaning that there's no root hash of it committed to anywhere, and that really creates quite a few issues in the short term, for example, already there's some uncertainties around how it interacts with block level access lists, because you basically have to indicate that the code size was read instead of the code itself. So you basically have to modify the format there, which is maybe possible. But then, once you go to Zk witnesses, which is something that we at least want to start doing very soon

335
00:56:13.980 --> 00:56:21.120
Ansgar Dietrichs: in parallel also this year, that basically becomes impossible because the Zk in Zk you need to be able to have a some sort of trace

336
00:56:21.120 --> 00:56:38.340
Ansgar Dietrichs: back to some trusted hash, and that just doesn't exist for this index. So within the zk proof you'd always have to load the full code. And even if then you actually run out of gas and can't actually use it. So that means that that further degrades zk performance. And in general, I personally think that this index has pretty severe forward compatibility.

337
00:56:38.340 --> 00:57:00.010
Ansgar Dietrichs: Considerations that I just don't see like they are not addressed at all in the eip, which to me means that actually, it's this late into the fog. I have severe headaches about it, but basically sorry this was a bit too long. But, like again, general point is, I really think we can't treat this index as an afterthought. We need to have standardized behavior across all clients, and agreed upon and agreed upon how we handle these concerns.

338
00:57:01.430 --> 00:57:09.540
jochem-brouwer: Yeah, I would also like to chime in on exactly like this index. What I would personally like to see is that we

339
00:57:09.740 --> 00:57:19.779
jochem-brouwer: encode this coda sites into the state tree, and I know this is like a very big thing, because that would alter the state tree. But if we would launch ethereum. Now

340
00:57:19.910 --> 00:57:34.279
jochem-brouwer: then, likely what we will do is we will put the code size into the account rop, which we would save into the state tree, so that what we will do is we now have 4 items in the account that will be the nonce, the balance, the code hash, and the state, the statehood

341
00:57:34.580 --> 00:57:54.130
jochem-brouwer: and what we will then do. If, if, for instance, we will relaunch ethereum, then we would add this extra item of coda size. And I notice that we yeah, we can't. We like, you can't really add something to the state tree now. But that would be the ideal and the most simple solution, I would say to this problem.

342
00:57:54.520 --> 00:57:57.489
jochem-brouwer: and this is also an idea for my gujang. By the way.

343
00:57:57.710 --> 00:58:00.120
jochem-brouwer: so thanks for that idea. Yeah.

344
00:58:04.620 --> 00:58:22.099
Ameziane Hamlat: Sorry, quick question we we already found some issues related to export hash, export size, and we are we fixed few of them, and we are working on on other issues related to code. Cache in general

345
00:58:22.400 --> 00:58:31.419
Ameziane Hamlat: would like to know if, when we push like these changes on the performance branch. How we can

346
00:58:31.610 --> 00:58:34.309
Ameziane Hamlat: trigger the like the same tests.

347
00:58:34.710 --> 00:58:38.099
Ameziane Hamlat: Can I just ping you and then run them again.

348
00:58:39.768 --> 00:58:42.210
jochem-brouwer: You mean as like a state test.

349
00:58:43.350 --> 00:58:46.495
Ameziane Hamlat: I mean the ones that you just shared related to

350
00:58:47.570 --> 00:58:48.190
Ameziane Hamlat: Yes.

351
00:58:48.190 --> 00:58:54.360
jochem-brouwer: Okay, yes, yes, I should add these tests to the to the fixtures, I think, to the state tests.

352
00:58:54.870 --> 00:58:55.839
jochem-brouwer: Would that work.

353
00:58:56.890 --> 00:59:06.000
Ameziane Hamlat: I'm not sure, because my question in the chat was, How do? How do we collect those results?

354
00:59:06.170 --> 00:59:14.090
Ameziane Hamlat: And Marius said that those results are from Perfnet, which is a Shadow Fork of Mainnet. From what I understood.

355
00:59:14.290 --> 00:59:14.930
jochem-brouwer: Yes.

356
00:59:16.140 --> 00:59:25.539
Ameziane Hamlat: And that's perfect. That's that's actually perfect, because like the the notes are are warmed up. So that that's perfect for Besu.

357
00:59:25.710 --> 00:59:32.270
Ameziane Hamlat: and would like actually to get the results again when we have the the fixes, like the performance fixes.

358
00:59:32.780 --> 00:59:52.819
jochem-brouwer: Okay, yeah, that's actually a very good point. Because also one of the reasons why we did this on a shadow fork is because we want this on like a very large database. So, for instance, like a fork of Mainnet, because if you will just do the state test, then your database size is well, not large enough, and we would really want to test this like on a big database like Mainnet. For that.

359
00:59:53.220 --> 01:00:03.369
jochem-brouwer: I think this is something for Panda Ops because they run these shadow forks, and then we likely have to update your client to a new version, and then we can run it again and see what the results are.

360
01:00:04.610 --> 01:00:05.770
Ameziane Hamlat: Okay. Thanks.

361
01:00:06.070 --> 01:00:07.599
jochem-brouwer: Yes, thank you.

362
01:00:12.090 --> 01:00:12.940
jochem-brouwer: I think it's also.

363
01:00:12.940 --> 01:00:14.870
Mario Vega: Point is that sorry.

364
01:00:15.060 --> 01:00:18.789
jochem-brouwer: And there's also ping me on discord if you need something. Yeah, thanks.

365
01:00:20.280 --> 01:00:41.509
Mario Vega: Thank you. Johan, if you could summarize somewhere the list of tests that were run in the Perfnet, that would be nice, because we can focus on either improving those or adding them to the to the releases to even if they are not as significant as when running. Imperfnet. I think it would be helpful to know which ones are. The ones are being run.

366
01:00:42.710 --> 01:00:46.020
jochem-brouwer: Yeah, I will add this to the to the east. Yeah. Good point.

367
01:00:49.420 --> 01:00:50.200
Mario Vega: Thank you.

368
01:00:50.580 --> 01:01:01.289
Mario Vega: Oh, alright! Any comments from the clients, or any anything that anyone else that wants to chime in on the results or the

369
01:01:01.540 --> 01:01:15.374
Mario Vega: I think I don't think we we touch on the spec side. I think one question would be, Johan, does this affect the current specification or do this is this effect? Does it require changes to the eip?

370
01:01:16.270 --> 01:01:19.570
Mario Vega: in any way that you feel that need to be done?

371
01:01:21.610 --> 01:01:23.389
jochem-brouwer: That's a good question

372
01:01:23.680 --> 01:01:33.460
jochem-brouwer: for that. I'm not really sure. I think that. Yeah, from the testing thing, there are only some points which might be considered unspecified. I think those should be specified.

373
01:01:33.830 --> 01:01:34.900
jochem-brouwer: And

374
01:01:35.520 --> 01:01:43.319
jochem-brouwer: yes, but for now I don't really have some some big things here to to add, yeah, these are just yeah, benchmark results. Yeah.

375
01:01:46.490 --> 01:02:05.690
Mario Vega: Okay, thank you. From the perspective. I don't think we're changing the the current test. I I it would be 2 last minute. So I think we can wait until tomorrow when definitely launches, and then, if it results that we need to change something to the eip, it would be it would it would need to go into that 3,

376
01:02:05.990 --> 01:02:07.260
Mario Vega: in my opinion.

377
01:02:10.820 --> 01:02:19.055
Mario Vega: alright and one last question from answer. How many yield clients looked into the code size index at all. So far.

378
01:02:19.990 --> 01:02:26.199
Mario Vega: are there any comments on any clients? Is this something that everyone has considered? Or what's the status

379
01:02:26.830 --> 01:02:28.280
Mario Vega: of the rest of the clients.

380
01:02:48.440 --> 01:02:55.189
Mario Vega: if not there's 1 comment from bargain Mouse, that technically should only have repricing of eaps.

381
01:02:55.550 --> 01:02:57.519
Mario Vega: Yeah, we should make sense.

382
01:03:02.870 --> 01:03:10.780
Roman: Well, well, we we agreed on acd. That 7, 9 0. 7 from light client will go in as well.

383
01:03:17.170 --> 01:03:21.199
Ben Adams: That was the tap on tap on contract size.

384
01:03:22.670 --> 01:03:23.490
Roman: Yeah.

385
01:03:27.890 --> 01:03:36.519
Mario Vega: Hey, yeah. And I just wanted to show me to say that if there's any changes that need to be done because we found significant performance. Well, next

386
01:03:36.720 --> 01:03:46.033
Mario Vega: is that is that a question of whether we continue with the current state of the eip, or does it need any modification. I think we we still have to discuss this. But yeah,

387
01:03:47.050 --> 01:03:49.900
Mario Vega: I'm not sure about the the answer to that. To be honest.

388
01:04:00.380 --> 01:04:01.210
Mario Vega: Yeah.

389
01:04:05.480 --> 01:04:10.770
Mario Vega: any other comments, about 79 0 7, I I still I still see a lot of shatter in the

390
01:04:11.110 --> 01:04:13.159
Mario Vega: our messages in the chat.

391
01:04:15.790 --> 01:04:35.109
jochem-brouwer: Yeah, maybe just to chime in. I think that for Defnet 2, the current state of the Erp is the one which is scheduled for Defnet 2. There is an unmatched Pr, which is what Lucas just posted. It's 9, 1, 9, 9, 1 0. That's like clients. Pr, I think that one was scheduled for Defnet 3.

392
01:04:35.250 --> 01:04:44.410
jochem-brouwer: But for definite 2. I think the current spec, and maybe also the fixtures which got released. I think that is the 256 kB limit of the Eap.

393
01:04:47.870 --> 01:04:57.660
Mario Vega: Yes, that is correct. We we have 256 kilobits. So the the fixtures right now in the tests are for the current version that is merged into master of the eip.

394
01:04:57.950 --> 01:05:05.350
Łukasz Rozmej: Yeah, I complained about it like 3 times already, and it doesn't make sense to to go with that fashion and and change. But

395
01:05:05.860 --> 01:05:08.550
Łukasz Rozmej: it was added already, and

396
01:05:08.800 --> 01:05:17.439
Łukasz Rozmej: you know that then this is a roadblock, and we cannot change it. It's my opinion this is worthless information, including it in the 2, 5, 6 version.

397
01:05:21.340 --> 01:05:28.920
jochem-brouwer: Okay. So I then open up Pr, which only changes this Max code size to 40 kB and

398
01:05:29.140 --> 01:05:31.579
jochem-brouwer: then use that specification for def net 2.

399
01:05:38.360 --> 01:05:41.389
Mario Vega: I think, in my opinion, it's too late to change anything for them to.

400
01:05:46.670 --> 01:05:47.410
Csaba K: Huh!

401
01:05:47.970 --> 01:05:49.120
Csaba K: No instant.

402
01:05:57.790 --> 01:06:07.129
Mario Vega: Yeah, I think I think letting it as is for them, too, I think, is is the the only thing that we can go do now and get more information once that definitely 2 is is out.

403
01:06:09.550 --> 01:06:10.340
Mario Vega: Yep.

404
01:06:15.410 --> 01:06:22.969
Mario Vega: Also, I think the Pr is for Matt of the eap. I think it would be nice if you can comment on it. But I think he's not in the call.

405
01:06:24.700 --> 01:06:28.480
Mario Vega: Yep, yeah, okay.

406
01:06:29.070 --> 01:06:36.139
Mario Vega: Sounds good. Alright. So I I think the the final point is that we we continue as this

407
01:06:36.260 --> 01:06:37.420
Mario Vega: 4 22.

408
01:06:37.750 --> 01:06:40.109
Mario Vega: So 256 kilowatts.

409
01:06:40.440 --> 01:06:46.690
Mario Vega: and the the current state of the master vip that is going to merge, and when we then we reassess

410
01:06:47.514 --> 01:06:53.909
Mario Vega: either in acde or i async, for the final changes to the eap.

411
01:06:56.640 --> 01:07:00.909
Mario Vega: Does that sound good by everyone, or any comments or counterpoints?

412
01:07:07.630 --> 01:07:08.390
Mario Vega: Cool?

413
01:07:08.860 --> 01:07:16.000
Mario Vega: Alright? So that's it. And we only have 2 min left. Are there any other comments or quick.

414
01:07:16.450 --> 01:07:20.769
Mario Vega: final thoughts that we should do before closing the call?

415
01:07:25.220 --> 01:07:31.639
Mario Vega: If not, thank you. Everyone for joining, and we see you on Acdc. On Thursday.

416
01:07:32.810 --> 01:07:33.720
Mario Vega: Thank you.

417
01:07:33.900 --> 01:07:34.660
Łukasz Rozmej: Bye.

418
01:07:35.000 --> 01:07:35.780
jochem-brouwer: Thank you.

419
01:07:37.760 --> 01:07:38.420
Ansgar Dietrichs: What's everyone?

