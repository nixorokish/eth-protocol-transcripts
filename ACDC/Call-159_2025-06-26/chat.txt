00:04:10	spencer-tb:	Gm! Ga!!
00:04:14	Barnabas:	Ansgar for governor
00:04:28	spencer-tb:	Reacted to "Ansgar for governor" with 🛳️
00:07:11	Ansgar Dietrichs:	https://github.com/ethereum/pm/issues/1579
00:09:59	nixo:	https://testinprod.notion.site/Sunnyside-Devnet-Updates-06-23-Internal-21b8fc57f54680639103d8b54c5f1500
00:13:00	Barnabas:	Please note, berlinterop-devnet-2 is going offline by end of today!
00:13:03	Ansgar Dietrichs:	https://github.com/ethereum/pm/issues/1579#issuecomment-3005918568
00:13:07	Manu:	Reacted to "Please note, berlint..." with 👍
00:13:26	J Sunnyside Labs:	Reacted to "https://testinprod.n..." with 👍
00:14:18	Ansgar Dietrichs:	First PR: https://github.com/ethereum/consensus-specs/pull/4393
00:14:28	Manu:	Peer sampling replaced by custody sampling
00:14:33	Francesco:	Reacted to "Peer sampling replac..." with 👍
00:14:42	Francesco:	Replying to "Peer sampling replac..."

Yeah I’d say it’s pretty orthogonal to validator custody
00:14:56	Barnabas:	Reacted to "Peer sampling replac..." with 👍
00:15:08	Barnabas:	Anyone opposing merging this in?
00:15:32	Dustin:	Replying to "Peer sampling repl..."

fair enough
00:15:38	Manu:	Replying to "Anyone opposing merg..."

Go nuke
00:15:44	Enrico Del Fante (tbenr):	Go delete
00:16:04	Ansgar Dietrichs:	Second PR: https://github.com/ethereum/consensus-specs/pull/4394
00:17:28	pawan:	In favour of merging this PR from lighthouse. It’ll be helpful for different sync strategies we are trying as well
00:17:48	Manu:	Reacted to "In favour of merging..." with 👍
00:17:50	kingy_sigp:	Reacted to "In favour of merging..." with 👍
00:17:58	kasey:	Reacted to "In favour of merging..." with 🎉
00:18:04	Barnabas:	Reacted to "In favour of merging..." with 🎉
00:18:25	Ansgar Dietrichs:	Third PR: https://github.com/ethereum/consensus-specs/pull/4406
00:19:01	Barnabas:	can we get a few more thumbs up on each PR pls?
00:19:12	Barnabas:	so its gonna be a bit nicer to merge them in
00:19:27	Manu:	Reacted to "First PR: https://gi..." with 👍
00:19:51	kingy_sigp:	Reacted to "First PR: https://gi..." with 👍
00:19:52	ethDreamer (Mark):	Oh lol you might have said “that has” and I heard “lighthouse” 😂
00:19:56	Enrico Del Fante (tbenr):	Good to go
00:20:04	kingy_sigp:	Reacted to "Third PR: https://gi..." with 👍
00:20:10	Barnabas:	sendit
00:21:24	Barnabas:	If its not in FULU it should be gone
00:21:50	Barnabas:	maybe we can have a “feature_request” spec, that are optional to implement
00:22:33	kingy_sigp:	if you're speaking for Jimmy from LH he's fine to lose it
00:22:50	Barnabas:	I personally think, if at least one client is implementing, then it should be kept.
00:23:19	Dustin:	Replying to "I personally think..."

is this the case though
00:23:25	Francesco:	Replying to "I personally think, ..."

No it’s not
00:23:32	Barnabas:	Replying to "I personally think, ..."

Jimmy said he might be looking into it
00:23:38	Francesco:	Replying to "I personally think, ..."

I don’t think for Fulu though?
00:23:50	pawan:	Replying to "I personally think, ..."

No we aren’;t implementing it either
00:23:52	Barnabas:	Replying to "I personally think, ..."

I’m not sure if it was for fulu
00:23:58	Barnabas:	Reacted to "No we aren’;t implem..." with 👍
00:24:01	Barnabas:	Replying to "I personally think, ..."

then yeet
00:24:19	Barnabas:	Replying to "I personally think, ..."

we can always reintroduce for future das versions
00:24:31	Ansgar Dietrichs:	Fourth PR: https://github.com/ethereum/consensus-specs/pull/4407
00:25:02	Barnabas:	Kasey approves
00:25:29	pawan:	Reacted to "Screenshot 2025-06-26 at 16.20.31.png" with 😁
00:25:32	kingy_sigp:	Reacted to "Fourth PR: https://g..." with 👍
00:25:39	Josh Davis:	Reacted to "Screenshot 2025-06-26 at 16.20.31.png" with 😁
00:25:41	Will Corcoran:	Reacted to "Screenshot 2025-06-26 at 16.20.31.png" with 😁
00:25:47	felipe:	Reacted to "Screenshot 2025-06..." with 😁
00:26:31	Enrico Del Fante (tbenr):	Reacted to "Screenshot 2025-06-26 at 16.20.31.png" with ❤️
00:26:38	nixo:	Reacted to "Screenshot 2025-06-26 at 16.20.31.png" with 😁
00:26:52	Barnabas:	ACDE - 4 new EIPS
ACDC - 4 PRs merged

Ansgar is on 🔥
00:26:54	Parithosh Jayanthi:	Reacted to "Screenshot 2025-06-26 at 16.20.31.png" with ❤️
00:26:58	Barnabé Monnot:	Reacted to "ACDE - 4 new EIPS
AC..." with 🔥
00:26:59	Parithosh Jayanthi:	Reacted to "ACDE - 4 new EIPS
AC..." with 🔥
00:27:02	felipe:	Reacted to "ACDE - 4 new EIPS
..." with 🔥
00:27:05	Enrico Del Fante (tbenr):	Reacted to "ACDE - 4 new EIPS
AC..." with 🔥
00:27:22	Caspar Schwarz-Schilling:	Reacted to "ACDE - 4 new EIPS
AC..." with 🔥
00:27:28	pawan:	Reacted to "ACDE - 4 new EIPS
AC..." with 🔥
00:27:36	Francesco:	Reacted to "Screenshot 2025-06-26 at 16.20.31.png" with 😁
00:27:47	Francesco:	Replying to "ACDE - 4 new EIPS
AC..."

Sounds like a yes man if you ask me
00:27:48	Rafael Matias:	Reacted to "Screenshot 2025-06-26 at 16.20.31.png" with 😁
00:27:57	Manu:	Reacted to "Screenshot 2025-06-26 at 16.20.31.png" with 😁
00:28:20	Will Corcoran:	Reacted to "ACDE - 4 new EIPS
AC..." with 🔥
00:28:23	felix (eest):	Reacted to "ACDE - 4 new EIPS
..." with 🔥
00:28:24	Marius van der Wijden:	Reacted to "Sounds like a yes ..." with 💯
00:28:32	Marius van der Wijden:	Reacted to "Screenshot 2025-06..." with 😁
00:28:44	Parithosh Jayanthi:	So with the PRs merged, we are ready for Fusaka?
00:28:54	Parithosh Jayanthi:	Pending findings while testing ofc
00:29:05	J Sunnyside Labs:	Reacted to "Screenshot 2025-06-26 at 16.20.31.png" with 😁
00:29:24	Barnabé Monnot:	Reacted to "Sounds like a yes ma..." with 🙆
00:29:35	Rafael Matias:	Reacted to "ACDE - 4 new EIPS
AC..." with 🔥
00:29:37	Parithosh Jayanthi:	Reacted to "Sounds like a yes ma..." with 🙆
00:29:42	Barnabas:	Reacted to "Sounds like a yes ma..." with 🙆
00:30:04	spencer-tb:	Reacted to "Sounds like a yes ma..." with 🙆
00:30:34	pawan:	Reacted to "So with the PRs merg..." with 👍
00:31:14	Parithosh Jayanthi:	LFGGG!
00:31:34	Francesco:	Time is over
00:31:36	lightclient:	isn’t that a fast timeline for devnet 3
00:31:41	Parithosh Jayanthi:	Reacted to "Time is over" with 🔥
00:31:41	Barnabas:	Ansgar ships
00:31:44	Marius van der Wijden:	Wbat
00:31:52	Marius van der Wijden:	*what does frozen mean?
00:31:58	lightclient:	Reacted to "*what does frozen me…" with 😂
00:31:58	Barnabas:	Replying to "*what does frozen me..."

no more CL changes
00:32:00	Will Corcoran:	Reacted to "Ansgar ships" with 🚢
00:32:02	Marius van der Wijden:	Reacted to "*what does frozen ..." with 😂
00:32:05	Toni Wahrstaetter:	Reacted to "Ansgar ships" with 🚢
00:32:13	Barnabas:	Replying to "isn’t that a fast ti..."

lets see how the repricing goes on the EL side
00:32:21	Parithosh Jayanthi:	Replying to "*what does frozen me..."

And we can spend money on more expensive devnets without worrying about specs changing every week
00:32:22	Barnabas:	Reacted to "Screenshot2025_06_26_162737.jpg" with 😂
00:32:22	Katya Riazantseva:	Reacted to "Screenshot2025_06_26_162737.jpg" with 😂
00:32:24	Parithosh Jayanthi:	Reacted to "Screenshot2025_06_26_162737.jpg" with 😂
00:32:29	Ansgar Dietrichs:	https://blog.sigmaprime.io/shipping-peerdas.html
00:32:31	thomasthiery:	Reacted to "Screenshot2025_06_26_162737.jpg" with 😂
00:32:35	Barnabé Monnot:	Reacted to "Screenshot2025_06_26_162737.jpg" with 😂
00:32:35	kev:	Reacted to "Screenshot2025_06_26_162737.jpg" with 😂
00:32:57	Barnabas:	BPO is a diff topic than spec freeze imo
00:33:00	Parithosh Jayanthi:	Reacted to "BPO is a diff topic ..." with 👍🏽
00:33:10	felipe:	Reacted to "Screenshot2025_06_..." with 😂
00:33:20	Caspar Schwarz-Schilling:	Reacted to "Screenshot2025_06_26_162737.jpg" with 😂
00:33:42	nflaig:	Reacted to "Francesco has sent y..." with 😂
00:33:55	Caspar Schwarz-Schilling:	Reacted to "Sounds like a yes ma..." with 🗿
00:34:44	Barnabas:	proposed BPO1 value was: 9/12
00:34:57	Ansgar Dietrichs:	Replying to "BPO is a diff topic ..."

right, sorry, I might not have clearly expressed that
00:35:11	pawan:	So ship Fusaka with same max_blobs as electra?
00:35:21	Barnabas:	Replying to "So ship Fusaka with ..."

yes
00:35:25	kasey:	Replying to "So ship Fusaka with ..."

+1
00:35:34	pawan:	Replying to "So ship Fusaka with ..."

Agree its better to be conservative
00:35:40	kingy_sigp:	Reacted to "https://blog.sigmapr..." with 👍
00:35:45	Manu:	Replying to "So ship Fusaka with ..."

+1
00:35:45	kingy_sigp:	Reacted to "Agree its better to ..." with 👍
00:35:48	Ansgar Dietrichs:	Replying to "proposed BPO1 value ..."

3:4 is a somewhat extreme ratio, I'd prefer 8:12 (if we want 12 max)
00:36:12	Barnabas:	Replying to "proposed BPO1 value ..."

also good, CL devs only care about MAX not target anyway 😄
00:36:38	Potuz:	Replying to ""

we care about target
00:36:40	Potuz:	Replying to ""

not max
00:36:48	Potuz:	Replying to ""

target is what determines the sync safety
00:36:54	Manu:	Replying to "proposed BPO1 value ..."

We care about both actually
00:36:56	Enrico Del Fante (tbenr):	Makes sense to just activate fulu and then rise later via BPOs. go slow at BPO1 and then be more aggressive later LGTM
00:37:01	Francesco:	Reacted to "We care about both a..." with 😄
00:37:03	Potuz:	Reacted to "We care about both a..." with 😄
00:37:07	Barnabas:	Replying to "proposed BPO1 value ..."

you read in the max only from the config 😄
00:37:15	Barnabas:	Replying to "proposed BPO1 value ..."

you have no idea what target is
00:37:27	Ansgar Dietrichs:	I personally would like to have 2 BPOs bundled with Fusaka:

Fusaka (6/9, no immediate bump)
BPO 1 (8/12, the week after)
BPO 2 (12/18, 1-2 months after)

that way, we would have proper time for analysis and rolling out the next batch of BPOs, without too much dead time on mainnet with still Pectra-like throughput values
00:38:05	Barnabas:	I’d do one testnet with same as mainnet tho
00:38:06	Enrico Del Fante (tbenr):	As long as testnets are more aggressive, it’s fine 🙂
00:38:09	pawan:	Reacted to "Makes sense to just ..." with 👍
00:38:10	Potuz:	Reacted to "I personally would l..." with 👍🏼
00:38:42	Manu:	Replying to "proposed BPO1 value ..."

We have no idea what the target is, agree, but the target, for example, has an impact on the total DB size for blobs.
00:38:42	Enrico Del Fante (tbenr):	Reacted to "I personally would l..." with 👍🏼
00:39:36	Manu:	Replying to "proposed BPO1 value ..."

And we care about max for P2P issues, disk write speed concerns etc…
00:41:09	Justin Florentine (Besu):	it's more about cutting a release overhead
00:41:24	Francesco:	Replying to "it's more about cutt..."

Yeah that’s what I meant mostly
00:41:32	Barnabas:	I think the mainnet release should have fulu and bpo1 baked in
00:41:34	Potuz:	You need a few weeks between announcement and upgrade
00:41:39	Justin Florentine (Besu):	Replying to "it's more about cutt..."

analysis plus 1-2 weeks
00:42:06	lightclient:	Reacted to "You need a few weeks…" with 👍
00:42:10	pawan:	Reacted to "You need a few weeks..." with 👍
00:42:40	Dustin:	Reacted to "You need a few wee..." with 👍
00:43:49	Barnabas:	I think it also depends on when we ship fulu :D
00:44:12	Marius van der Wijden:	I would bake in the 2 first bpos in fusaka, do the rest later
00:44:19	lightclient:	Reacted to "I would bake in the …" with 👍
00:44:26	Ansgar Dietrichs:	Reacted to "I would bake in the ..." with 👍
00:44:28	Rafael Matias:	Reacted to "I would bake in the ..." with 👍
00:44:36	Carl Beekhuizen:	Reacted to "I would bake in the …" with 👍
00:44:46	J Sunnyside Labs:	Reacted to "I would bake in the ..." with 👍
00:45:06	Potuz:	Reacted to "I would bake in the ..." with 👍
00:45:08	Matthew Keil:	We should only bake in 1.  There is no replacement for network traffic to tax a node and its important to see the effects on overall node health before we raise aggressively
00:45:23	Justin Florentine (Besu):	i'm mostly aligned with pari, am open to 2 bpos included, but i do think the decision on how many to include is informed by how the first one goes.
00:45:24	Parithosh Jayanthi:	options:
Bake in BPO1
Bake in BPO1&2
Bake in BPO[1-5] and then abort if needed
00:45:32	Csaba Kiraly:	I would bake in 1
00:45:34	Barnabas:	We gonna do that on testnet only not on mainnet
00:46:00	Matthew Keil:	Testnet traffic is insufficient to really see how a node will perform
00:46:07	Potuz:	Replying to "I would bake in the ..."

I agree with this with the caveat that the second BPO should be scheduled with several weeks/months from the first so as to be able to roll it back if the first one doesn't work well
00:46:18	lightclient:	Replying to "I would bake in the …"
^^^
00:46:19	Enrico Del Fante (tbenr):	Replying to "We should only bake ..."

Unless there is at least 1 month between BPO1 and 2
00:46:21	Justin Florentine (Besu):	what if we bake in 2 and bpo 2 is a return to pectra blob rate?
00:46:26	Dustin:	if a BPO has gone wrong it's kind of too late, depending on how it goes wrong. The moral hazard here is people looking at a rollback built in and seeing it as license to be too aggressive
00:46:38	Barnabas:	Replying to "what if we bake in 2..."

would really not want to have this on mainnet.
00:47:01	pawan:	Replying to "what if we bake in 2..."

Why would we want to do this?
00:47:20	Justin Florentine (Besu):	Replying to "what if we bake in 2..."

i mean if bpos are up only why are we testing them on devnets?
00:47:39	Barnabas:	Replying to "what if we bake in 2..."

The mechanism has to work.
00:47:50	Justin Florentine (Besu):	Replying to "what if we bake in 2..."

why if we never use it in prod?
00:48:20	Csaba Kiraly:	For the 2nd bum, I think it is better to have a target we aim for both in number (2x) and timeline (2 months), but evaluate it before committing to the number.
00:48:22	Barnabas:	Replying to "what if we bake in 2..."

I think it could in theory be triggered
00:48:25	Francesco:	If BPOs are scheduled gradually enough, it should be possible to do it? Like, if it takes 2 months for throughput to go from fine to not fine
00:48:52	Francesco:	Replying to "If BPOs are schedule..."

But yes, don’t think we should schedule values that we are not quite comfortable with
00:48:55	pawan:	Replying to "If BPOs are schedule..."

Depends on the numbers too
00:49:00	Csaba Kiraly:	Replying to "For the 2nd bum, I t..."

That allows us to be both more conservative and more aggressive based on tests. Tests that we don’t have at the moment.
00:49:04	Barnabas:	Replying to "For the 2nd bum, I t..."

how do you evaluate something that you have no way to verify ?
00:49:33	Csaba Kiraly:	Replying to "For the 2nd bum, I t..."

We can do much more verification that we’ve dine till now, that’s clear.
00:50:18	Justin Florentine (Besu):	@Ansgar Dietrichs you have hands to call on
00:50:34	Marius van der Wijden:	I think we should test it...
00:50:46	spencer-tb:	Reacted to "I think we should te..." with ➕
00:50:57	Parithosh Jayanthi:	Reacted to "I think we should te..." with ➕
00:50:59	Ansgar Dietrichs:	Replying to "@Ansgar Dietrichs yo..."

oh, sorry
00:51:04	lightclient:	Reacted to "I think we should te…" with ➕
00:51:10	ethDreamer (Mark):	Sorry maybe I don’t understand the proposal
00:51:27	kasey:	Replying to "If BPOs are schedule..."

People have been talking about bumping minimum 3 at a time, but I do prefer the idea of a schedule with many single blob increases vs fewer 3-at-a-time increases.
00:51:37	pawan:	Replying to "I think we should te..."

Test reducing the max_blobs? We are already doing that to test the mechanism
00:52:21	Marius van der Wijden:	Replying to "I think we should ..."

I agree with you Dustin, that we should not think of reducing the BPO as anything but an emergency fork
00:52:36	pawan:	Reacted to "I agree with you Dus..." with 👍
00:53:23	Ansgar Dietrichs:	right, so for clarity, these are separate topics
cancel a pre-scheduled BPO fork, by having new client releases without that BPO
roll back a BPO fork after it happened, by having a new release with another BPO fork that returns back to old / otherwise lower parameters
00:53:35	Potuz:	Reacted to "right, so for clarit..." with 👍🏼
00:54:49	Parithosh Jayanthi:	Reacted to "I agree with you Dus..." with 👍
00:56:24	Justin Florentine (Besu):	Replying to "I think we should te..."

if you think of it as an emergency procedure, i don't think testing it on devnets is sufficient to trust it for mainnet.
00:56:37	kasey:	This raises an issue with the nfd disconnection behavior. As the network rolls out a bpo schedule change, nodes will disagree on the nfd.
00:56:51	kasey:	Replying to "This raises an issue..."

We don’t want to fragment in this scenario.
00:56:55	Justin Florentine (Besu):	Replying to "I think we should te..."

it's like testing your life preserver by throwing it in the tub. then taking it out to sea.
00:57:27	Potuz:	the original motivation was to rearrange the slot
00:57:37	Ansgar Dietrichs:	PR: https://github.com/ethereum/consensus-specs/pull/3510
00:57:38	Francesco:	Replying to "This raises an issue..."

But they wouldn’t disconnect until the bpo fork epoch, no?
00:58:35	kasey:	Replying to "This raises an issue..."

Imagine you are the node that has not upgraded and you’re at the end of the schedule. You disagree with a potential peer’s nfd and the epoch in question has already passed, so you would disconnect.
00:59:14	Barnabas:	Replying to "PR: https://github.c..."

anyone opposed merging this in before we hit fulu?
00:59:34	Csaba Kiraly:	@Justin Florentine (Besu) Indeed, the test there is not what we want.
What we would need is to first drive it over the limit, and then try to save it by reducing it.
The problem is that “over the limit” can mean many possible failure modes, and some of those we can’t test, or at least we did not yet thing thrugh how to test.
00:59:40	Francesco:	Replying to "This raises an issue..."

Right but isn’t that the behaviour we want at that point? It would be fine if the bpo schedule change was rolled out sufficiently in advance
00:59:52	kasey:	Reacted to "@Justin Florentine (..." with 👍
01:00:14	Barnabas:	Replying to "This raises an issue..."

nfd issues would be raised during devnets
01:00:29	Barnabas:	Replying to "This raises an issue..."

highly unlikely we would have an emergency release faster than 256 epochs
01:01:07	Justin Florentine (Besu):	Replying to "I think we should te..."

i understand that, i just don't want to be in a situation where we need to consider bpo limit going down, and we have only tested the mechanic on devnets. devnets are very contrived.
01:01:44	Potuz:	there's no real reason to have this pre-fulu so it seems reasonable to me to have this for Glamsterdam branches
01:01:50	pawan:	Reacted to "there's no real reas..." with 👍
01:02:03	Potuz:	agreeing with @Barnabas for the first time in my life
01:02:10	Parithosh Jayanthi:	Reacted to "agreeing with @Barna..." with 😂
01:02:12	felix (eest):	Reacted to "agreeing with @Bar..." with 😂
01:02:15	Barnabé Monnot:	Reacted to "agreeing with @Barna..." with 🤝
01:02:29	Barnabas:	Reacted to "agreeing with @Barna..." with 😂
01:02:34	kasey:	Replying to "This raises an issue..."

Isn’t that the behavior we want
I suppose we would set the epoch of the bpo to reduce to a value right after the epoch where the true limit was overshot, so you’re probably right.
01:02:42	Barnabas:	Replying to "agreeing with @Barna..."

either I’m becoming more wise, or you starting to lose your edge 😂
01:02:52	Potuz:	Reacted to "either I’m becoming ..." with 🤣
01:02:55	Enrico Del Fante (tbenr):	Reacted to "agreeing with @Barna..." with 😂
01:02:57	Parithosh Jayanthi:	Reacted to "either I’m becoming ..." with 🤣
01:03:06	Justin Florentine (Besu):	Replying to "I think we should te..."

because what is gonna happen in the happiest situation, is that home stakers can't keep up with blob propagation, and they complain. if bpo is up only, we have no option to help them.
01:03:07	Rafael Matias:	Reacted to "agreeing with @Barna..." with 😂
01:03:27	Csaba Kiraly:	Replying to "I think we should te..."

I agree with you. What I was saying is that going down is not tested currently for the situation we would try to use it for.
01:03:33	Barnabas:	glamsterdam be built on top of 3510 basically
01:04:02	Justin Florentine (Besu):	Reacted to "I agree with you. Wh..." with 🤝
01:04:13	Francesco:	Reacted to "either I’m becoming ..." with 🤣
01:04:18	Csaba Kiraly:	Reacted to "agreeing with @Barna..." with 😂
01:04:26	Csaba Kiraly:	Reacted to "either I’m becoming ..." with 🤣
01:04:33	kasey:	Reacted to "if you think of it a..." with 💯
01:06:07	Ansgar Dietrichs:	Reacted to "glamsterdam be built..." with 👍
01:06:09	Parithosh Jayanthi:	Reacted to "glamsterdam be built..." with 👍
01:06:46	Barnabas:	is the plan to half the attestation rewards with halving the slot time?
01:06:56	Ben Adams:	Reacted to "is the plan to half ..." with 👍
01:06:58	Ansgar Dietrichs:	Replying to "is the plan to half ..."

yes
01:07:14	Ansgar Dietrichs:	Replying to "is the plan to half ..."

basically, idea is always to keep rewards per time constant
01:07:22	Dankrad Feist:	Reacted to basically, idea is a... with "👍"
01:07:28	Toni Wahrstaetter:	Reacted to "basically, idea is a..." with 👍
01:07:51	Potuz:	Replying to "is the plan to half ..."

can we sneak also a lower issuance in the EIP?
01:07:59	lightclient:	is 12->6 sec reduction enough to be that useful for confirmation? 6 sec is still fairly slow, it doesnt remove the need for faster out of protocol confirmations
01:08:02	Barnabas:	Replying to "is the plan to half ..."

that would kill it 😄
01:08:03	DA | Flashbots:	Reacted to "can we sneak also a ..." with 👀
01:08:13	Will Corcoran:	Reacted to "can we sneak also a ..." with 👀
01:08:25	lightclient:	Reacted to "can we sneak also a …" with 👀
01:08:35	thomasthiery:	Reacted to "can we sneak also a ..." with 👀
01:09:21	Manu:	Does it go along with gas limit halving? (If not, we are going to x2 gas/sec)
01:09:36	Justin Florentine (Besu):	Reacted to "Does it go along wit..." with ➕
01:09:39	Barnabas:	Replying to "Does it go along wit..."

yes
01:09:40	Ansgar Dietrichs:	is this a sneak proposal for having canonical emojis for hard forks?
01:09:48	Francesco:	Replying to "Does it go along wit..."

Gas/s should stay constant (or slightly lower) unless we happen to think we want to increase it at the same time, for other reasons
01:09:48	Will Corcoran:	Reacted to "is this a sneak prop..." with 😃
01:09:56	Marius van der Wijden:	We would also need to half gas limit and half #blobs per block. Halving blocktimes also means we might not have some of the amortization effects that we benefit from right now
01:10:11	Francesco:	Reacted to "We would also need t..." with 👍
01:10:16	Ansgar Dietrichs:	Reacted to "Does it go along wit..." with 👍
01:10:28	Ben Adams:	Reacted to "is this a sneak prop..." with 😃
01:10:30	Ansgar Dietrichs:	Reacted to "We would also need t..." with 👍
01:10:35	Potuz:	Reacted to "is this a sneak prop..." with 😃
01:10:37	Barnabas:	we even gonna be able to test BPO value reduction in prod :D
01:10:37	Marius van der Wijden:	Replying to "We would also need..."

On the other hand, some worst cases grow superlinear with gas limit
01:10:48	Katya Riazantseva:	for shorter slots metrics specs seem to be even more valuable. Testing will be tough
01:11:03	Ansgar Dietrichs:	Replying to "We would also need t..."

right, shorter slots are explicitly never a scaling strategy. if anything, they will usually slightly lower throughput overall
01:11:04	Justin Florentine (Besu):	Replying to "Does it go along wit..."

wat. that needs to be explicitly mentioned pls.
01:11:11	DA | Flashbots:	Reacted to "right, shorter slots..." with 👍
01:11:26	Ansgar Dietrichs:	Replying to "We would also need t..."

I am not convinced there are enough such superlinear cases for that argument to really matter
01:11:27	Ben Adams:	Replying to "We would also need t..."

Proposal includes halving everythingSo same per second
01:11:32	Csaba Kiraly:	I think shorter slot times are more about granularity than anything else. Most throughput like metrics are not changed. It is the granularity and as a consequence inclusion delay that gains.
On the flip side there is the overhead generated by this granularity.
01:11:41	Justin Florentine (Besu):	Reacted to "Proposal includes ha..." with ➕
01:11:42	Parithosh Jayanthi:	Reacted to "is this a sneak prop..." with 😃
01:11:48	lightclient:	Reacted to "I think shorter slot…" with 👍
01:11:51	Barnabas:	I think 6s slot time gonna happen sooner or later, the question is do we want epbs/delayed execution before or after 6s slot time. (Doubt we can do both in the same fork)
01:12:10	DA | Flashbots:	What’s the evidence you’re using that cutting the first part of the slot from 4s —> 3s is something local builders can handle?
01:12:16	Parithosh Jayanthi:	^ also if the specs and changes are understood well enough for a fork that can happen ~6mos after fusaka
01:12:24	Anders Kristiansen:	given we are moving towards enshrined zkEVM, this should be aligned with real-time proving, no?
01:12:28	Marius van der Wijden:	Reacted to "^ also if the spec..." with 👍
01:12:32	Dankrad Feist:	Replying to "is 12->6 sec reducti..." 

 It's definitely already a major step for several metrics but part of the value would also signalling that we care about this and will continue trying to lower slot times as far as possible
01:12:52	Sophia Gold:	Replying to "given we are movin..."

There's no significant conflict
01:12:59	Barnabé Monnot:	Reacted to "It's definitely alre..." with 👍
01:13:04	Carl Beekhuizen:	Reacted to "It's definitely alre..." with 👍
01:13:10	Manu:	Reacted to "Proposal includes ha..." with 👍
01:13:11	Julian Ma:	Replying to "What’s the evidence …"
The throughput would be halved and the propagation time less than halved so I don’t see a clear conflict for local block builders, in fact it may be easier to propagate a full block as you have more time for less data
01:13:12	Dankrad Feist:	Replying to "Does it go along wit..." 

 It is explicit in the EIP
01:13:20	Dankrad Feist:	Reacted to Gas/s should stay co... with "👍"
01:13:24	Marius van der Wijden:	Replying to "given we are movin..."

Why? It would require another 2x from provers?
01:13:35	Dankrad Feist:	Reacted to Proposal includes ha... with "👍"
01:13:39	Julian Ma:	Replying to "What’s the evidence …"
There is of course overhead which we would need to study better
01:13:39	lightclient:	which order is currently leading
01:13:48	Justin Florentine (Besu):	Reacted to "It is explicit in th..." with 👍
01:14:11	Manu:	Reacted to "It is explicit in th..." with 👍
01:14:29	Ansgar Dietrichs:	we can say ePBS* (pronounced epbs-star) instead of “slot restructuring”, meaning ePBS or something close to it :-)
01:14:33	Julian Ma:	Replying to "given we are moving …"
Time required for proving is pretty much linear in the gas used in a block. Since this proposal doesn’t increase gas/s, proving time isn’t affected
01:14:36	Sophia Gold:	Replying to "given we are movin..."

No, it wouldn't. They're only slightly sublinear in gas per block
01:14:37	Csaba Kiraly:	I think shorter slot times should come with two important techniques:
Pipelining
Techniques to make reorg or skipped slots cheaper
This is because shorter slot times by definition increase the probability of such events happening, so we should reduce the relative cost of these.
01:14:45	Marius van der Wijden:	Replying to "which order is cur..."

Not a CL dev, but I think slot restructuring should go first before slot shortening
01:14:47	Sophia Gold:	Reacted to "Time required for ..." with 👍
01:14:51	Potuz:	Reacted to "we can say ePBS* (pr..." with 🤣
01:14:51	DA | Flashbots:	Reacted to "There is of course o..." with 👍
01:15:11	Dankrad Feist:	Replying to "What’s the evidence ..." 

 Can you explain why they could not handle it? 3s should be plenty of time to build and propagate, afaict all of the missed slots are due to blocks being built late and not local builders who build on time
01:15:57	Barnabas:	Have we considered doing anything other than 6s?
01:16:11	Sophia Gold:	Replying to "I think 6s slot ti..."

Delayed execution shouldn't conflict with shorter slot times
01:16:15	Ansgar Dietrichs:	can’t wait to call on myself in a sec
01:16:22	nixo:	Reacted to "can’t wait to call o..." with 😂
01:16:23	Will Corcoran:	Reacted to "can’t wait to call o..." with 😂
01:16:31	Parithosh Jayanthi:	Reacted to "can’t wait to call o..." with 😂
01:16:34	Marius van der Wijden:	Replying to "Have we considered..."

Lets do 10 minutes
01:16:36	Barnabas:	Replying to "I think 6s slot time..."

In terms of code change it does
01:16:38	pawan:	Reacted to "can’t wait to call o..." with 😂
01:16:50	Marius van der Wijden:	Replying to "Have we considered..."

Parity with BTC
01:16:52	ethDreamer (Mark):	Reacted to "can’t wait to call o..." with 😂
01:17:11	Barnabé Monnot:	Why does shorter slot times decrease network resiliency? assuming there is p99 confidence on things arriving on time
01:17:18	Sophia Gold:	Replying to "I think 6s slot ti..."

Why?
01:17:19	DA | Flashbots:	Replying to "What’s the evidence ..."

imo the pipeline is slower than people realize + overhead from mevboost, so this probably kills any form of min-bid etc.
01:17:25	lightclient:	Reacted to "can’t wait to call o…" with 😂
01:17:35	DA | Flashbots:	Replying to "What’s the evidence ..."

local builders look much healthier than they actually are because of how small their blocks are.
01:17:37	Dankrad Feist:	Replying to "I think shorter slot..." 

 missed slots only incur an opportunity cost, no penalty. Since the proposal will halve gas and rewards it will also halve the opportunity cost
01:18:00	Francesco:	Replying to "I think 6s slot time..."

In principle delayed execution can be an EL change only, which doesn’t interact much with shorter slot times. On the other hand, right now most of the slot restructuring proposals are actually CL heavy
01:18:01	thomasthiery:	If Glamsterdam discussions have started I’d like to also encourage people to think about FOCIL fits in the slot restructuring/slot shortening picture 

fwiw I think it’s compatible with both, but it can be considered part of slot restructuring imo
01:18:24	Potuz:	Reacted to "If Glamsterdam discu..." with 👍🏼
01:18:27	Julian Ma:	Reacted to "If Glamsterdam discu…" with 👍🏼
01:18:29	Sophia Gold:	FYI, Vitalik's suggestion for testing this was a network with existing geographically distributed ethstaker types showing they don't miss more attestations
01:18:31	Dankrad Feist:	Replying to "Have we considered d..." 

 the original proposal was 8s, but I think it's not ambitious enough. Right now it looks like 6s should just work out of the box
01:18:35	Justin Florentine (Besu):	YES. lots of interactions between slot/de/focil
01:18:42	Barnabé Monnot:	To mark’s point on whether 7732 is the optimal form for shorter slots: imo it's likely close, I want to increase my confidence on it however
01:18:52	Sophia Gold:	Reacted to "In principle delay..." with 👍
01:18:55	Potuz:	Replying to "If Glamsterdam discu..."

+1  I think any decision that is made that changes the slot should be made in the same fork or at least forward compatible with FOCIL
01:18:58	Toni Wahrstaetter:	pipelining + shorter slots 🤝

The question will be, do we want/need max pipelining in the form of ePBS which also comes with significant complexity, or are we fine with getting 70-80% of the pipelining - doing the delayed exec 7886 proposal. 
7886 in its current design could be a small step towards full ePBS, removing PTC, unconditional payment and block-payload separation.
01:19:08	Parithosh Jayanthi:	Replying to "FYI, Vitalik's sugge..."

We already have this via xatu data, there’s no need for another network
01:19:15	lightclient:	is the ux that noticeable for users though?
01:19:25	Parithosh Jayanthi:	Replying to "FYI, Vitalik's sugge..."

Or is this for testnets? You run into the issue of needing larger state/etc to be realistic
01:19:32	Potuz:	Paying less on tx sounds like a lot of UX benefit to me
01:19:38	Marius van der Wijden:	Reacted to "is the ux that not..." with 👍
01:19:42	mpaulucci | ethrex:	Reacted to "is the ux that notic..." with 👍
01:19:50	lightclient:	epbs makes txs cheaper?
01:19:58	Dankrad Feist:	Reacted to is the ux that notic... with "👍"
01:19:59	thomasthiery:	Reacted to "+1  I think any deci..." with 👍
01:19:59	Barnabé Monnot:	Replying to "is the ux that notic..."

this is one of many other benefits. but yes it is also quite visible. users wait 2-4 seconds before thinking sth is wrong. this puts the average inclusion latency around these numbers
01:20:00	Potuz:	yes
01:20:02	Ansgar Dietrichs:	Replying to "can’t wait to call o..."

oof having an opinion section now really ruins this host thing
01:20:16	Potuz:	you get to have many more blobs and higher gas limit on execution
01:20:17	Barnabé Monnot:	Reacted to "We already have this..." with 🔥
01:20:45	lightclient:	ah okay i see
01:20:49	Dankrad Feist:	Replying to "is the ux that notic..." 

 I think halving confirmation time is very noticable? 

by this argument, we should just do no scaling or slot time assumptions because none of them are enough?
01:20:51	Potuz:	Replying to "epbs makes txs cheap..."

^^^^^^
01:20:54	Sophia Gold:	Replying to "FYI, Vitalik's sug..."

I think the idea was a long running testnet. It would be significant work, but I assume some people may not be convinced by the xatu data alone
01:20:56	Barnabé Monnot:	Replying to "Paying less on tx so..."

it’s not either/or 🙂
01:20:58	Ansgar Dietrichs:	Replying to "epbs makes txs cheap..."

it (unironically) does! like any slot restructuring though
01:21:09	Ansgar Dietrichs:	Replying to "epbs makes txs cheap..."

it does it quite well though
01:21:17	Parithosh Jayanthi:	Reacted to "Screenshot2025_06_26_171603.jpg" with 😂
01:21:25	Dankrad Feist:	Replying to "is the ux that notic..." 

 I think we should 6s first, then 4s, and then an upgraded consensus that gets us to 1s eventually
01:21:41	Sophia Gold:	Replying to "epbs makes txs che..."

Does anything include the problems with PBS?
01:22:16	Barnabé Monnot:	Reacted to "+1  I think any deci..." with 👍
01:22:37	kev:	Reacted to "Screenshot2025_06_26_171603.jpg" with 😂
01:23:21	Dankrad Feist:	I think talking to many stakeholders, shorter slot times bring the more immediate benefits vs scaling as ansgar said

(eventually need to do both)
01:23:59	felix (eest):	Reacted to "Screenshot2025_06_..." with 🪚
01:24:00	felix (eest):	Reacted to "Screenshot2025_06_..." with 😂
01:24:16	Potuz:	There are actual contracts that need to change, Terence identified the Arbitrum contract
01:24:27	Barnabas:	EL explorers don’t care.

BN explorers are few and rare, Dora will probably support it before any CL supports it :D
01:24:32	Ansgar Dietrichs:	Replying to "I think talking to m..."

maybe we’d need to bring some of these stakeholders into acd then or have some sort of breakout on that topic, because I think the need / benefits of shorter slots have historically not been super legible to the core dev process
01:24:57	Julian Ma:	Reacted to "maybe we’d need to b…" with 👍
01:25:37	Potuz:	FWIW PReston is already working on a branch to shorten slot times
01:25:43	Potuz:	it's a lot of work but doable
01:25:51	Potuz:	it touches a lot, a lot of things
01:25:56	Barnabas:	Reacted to "FWIW PReston is alre..." with 👀
01:26:06	Barnabas:	Replying to "FWIW PReston is alre..."

wen shorter slot-time devnet?
01:26:25	Barnabé Monnot:	would like to understand really how much work it is for all stakeholders, if we did decide to do shorter slot times first, is this really unmanageable?
01:26:42	Barnabé Monnot:	Replying to "would like to unders..."

and how much “wasted” effort vs restructure -> shorten
01:27:27	Parithosh Jayanthi:	Wouldn’t we also need a new proposer boost value? Or is that independent?
01:27:33	Francesco:	Replying to "Wouldn’t we also nee..."

independent
01:28:02	Saulius Grigaitis | Grandine:	Feels like that oneliner attestations change in Pectra
01:28:10	Barnabas:	whats the impl time of 
epbs then lower slot time
vs
lower slot time then epbs?
01:28:14	Dustin:	Reacted to "Feels like that on..." with 👍
01:28:16	pawan:	Reacted to "Feels like that onel..." with 👍
01:28:34	Dustin:	Other general point, it's not just "shorter" slot time, it's variable slot time. Doubling the slot time would be similarly difficult
01:28:43	Francesco:	Replying to "Feels like that onel..."

Well I don’t think anyone is saying this is a one liner 😄
01:29:23	Francesco:	Replying to "Other general point,..."

Slot-Time-Only-Fork
01:29:31	Potuz:	Reacted to "Feels like that onel..." with 👍
01:29:32	Saulius Grigaitis | Grandine:	Replying to "Feels like that onel…"
No one calling it one liner because we has such small change in the last hardfork :D
01:29:37	Julian Ma:	Reacted to "Slot-Time-Only-Fork" with 😂
01:29:49	Parithosh Jayanthi:	Reacted to "Slot-Time-Only-Fork" with 😂
01:30:26	Sophia Gold:	If we can do both delayed execution and 6s slot times in glamsterdam, is ePBS even necessary?
01:30:34	Dankrad Feist:	Reacted to independent with "👍"
01:31:14	Barnabé Monnot:	why isn’t there testing twice though? test restructure, test shorter slots, vs test shorter slots, test restructure? not super obvious to me but maybe missing sth
01:31:24	kev:	Reacted to "Slot-Time-Only-Fork" with 😂
01:31:44	Potuz:	Replying to "If we can do both de..."

ePBS is delayed execution
01:31:46	Barnabas:	is epbs + 6 slot slot time in glamsterdam really fully out of the picture?
01:31:56	ethDreamer (Mark):	Replying to "why isn’t there test..."

There are several deadlines in a slot - attestation deadline - proposal - aggregation
01:31:57	Barnabas:	that way we have one slot restructuring with 6s slot time in mind
01:32:19	Potuz:	Replying to "is epbs + 6 slot slo..."

Hell yes
01:32:19	ethDreamer (Mark):	Replying to "why isn’t there test..."

We will need pandaops to measure each of these
01:32:20	Sophia Gold:	Replying to "If we can do both ..."

We are also debating doing EL-only delayed execution in glamsterdam
01:32:22	Barnabé Monnot:	Replying to "is epbs + 6 slot slo..."

i would become a bigger ePBS bull than potuz if it was true! but i don’t think it is 🙁
01:32:33	Francesco:	Replying to "is epbs + 6 slot slo..."

Imo if we decided that we really want shorter slot times, we should probably do that + delayed execution on the EL, rather than create a monster feature on the CL
01:32:35	ethDreamer (Mark):	Replying to "why isn’t there test..."

If we do restructuring after you measure them, we will change the physics of the slot
01:32:39	ethDreamer (Mark):	Replying to "why isn’t there test..."

Forcing you to measure again
01:32:40	Sophia Gold:	Reacted to "Imo if we decided ..." with 💯
01:32:43	Josh Davis:	Great job Ansgar!
01:32:55	Parithosh Jayanthi:	Great job 😄
